{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5373f9a0-efd6-49bd-af99-ddafd3be85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a29a72-2b01-48aa-bd6d-0e050669991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Professional/Desktop/dostNN/pretrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba16efeb-5ecc-4925-86a5-7f0452959f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretr = pd.read_csv(path + \"lenta-ru-news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a581f7bf-20da-497f-bc18-e2eac159cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 50000/50000 [1:15:50<00:00, 10.99it/s]\n"
     ]
    }
   ],
   "source": [
    "##Preprocess training data\n",
    "txt = ''\n",
    "for i in tqdm(range(50000)):\n",
    "    txt += pretr.loc[i].text + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1d544-dc18-4208-bee9-2eff17ec6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = pretr.text.str.cat(sep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6730f84c-0f7c-418b-aef9-f87aa765c113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63214472"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e34a4af3-2573-4ec5-a91d-574d20e7335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Вице-премьер по социальным вопросам Татьяна Голикова рассказала, в каких регионах России зафиксирована наиболее высокая смертность от рака, сообщает РИА Новости. По словам Голиковой, чаще всего онкологические заболевания становились причиной смерти в Псковской, Тверской, Тульской и Орловской областях, а также в Севастополе. Вице-премьер напомнила, что главные факторы смертности в России — рак и болезни системы кровообращения. В начале года стало известно, что смертность от онкологических заболеваний среди россиян снизилась впервые за три года. По данным Росстата, в 2017 году от рака умерли 289 тысяч человек. Это на 3,5 процента меньше, чем годом ранее. Австрийские правоохранительные органы не представили доказательств нарушения российскими биатлонистами антидопинговых правил. Об этом сообщил посол России в Вене Дмитрий Любинский по итогам встречи уполномоченного адвоката дипмиссии с представителями прокуратуры страны, передает ТАСС. «Действует презумпция невиновности. Каких-либо ограни'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Проверка целостности данных\n",
    "txt[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89d046fc-c140-4652-9916-4673b7fa3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\",\"\n",
      "\",\" \",\"!\",\"\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\"-\",\".\",\"/\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"@\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"[\",\"\\\",\"]\",\"^\",\"_\",\"`\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"{\",\"|\",\"}\",\"~\",\" \",\"¡\",\"£\",\"¥\",\"©\",\"«\",\"­\",\"®\",\"°\",\"±\",\"²\",\"·\",\"»\",\"¾\",\"Á\",\"Å\",\"Ç\",\"È\",\"É\",\"Ö\",\"×\",\"Ø\",\"ß\",\"à\",\"á\",\"â\",\"ã\",\"ä\",\"å\",\"æ\",\"ç\",\"è\",\"é\",\"ê\",\"ë\",\"ì\",\"í\",\"î\",\"ï\",\"ñ\",\"ò\",\"ó\",\"ô\",\"ö\",\"÷\",\"ø\",\"ú\",\"û\",\"ü\",\"ý\",\"ā\",\"ă\",\"ą\",\"ć\",\"Č\",\"č\",\"Đ\",\"ē\",\"ė\",\"ę\",\"ě\",\"ğ\",\"Ģ\",\"ī\",\"İ\",\"ı\",\"Ł\",\"ł\",\"ń\",\"ņ\",\"ŏ\",\"ő\",\"Ş\",\"ş\",\"Š\",\"š\",\"ţ\",\"ū\",\"ż\",\"Ǝ\",\"ɛ\",\"ʼ\",\"˚\",\"ˮ\",\"́\",\"̆\",\"̈\",\"Β\",\"Δ\",\"Λ\",\"Μ\",\"Σ\",\"Φ\",\"α\",\"β\",\"ε\",\"ι\",\"κ\",\"λ\",\"ο\",\"π\",\"ς\",\"σ\",\"φ\",\"ό\",\"Ё\",\"Є\",\"І\",\"Ї\",\"А\",\"Б\",\"В\",\"Г\",\"Д\",\"Е\",\"Ж\",\"З\",\"И\",\"Й\",\"К\",\"Л\",\"М\",\"Н\",\"О\",\"П\",\"Р\",\"С\",\"Т\",\"У\",\"Ф\",\"Х\",\"Ц\",\"Ч\",\"Ш\",\"Щ\",\"Ъ\",\"Ы\",\"Ь\",\"Э\",\"Ю\",\"Я\",\"а\",\"б\",\"в\",\"г\",\"д\",\"е\",\"ж\",\"з\",\"и\",\"й\",\"к\",\"л\",\"м\",\"н\",\"о\",\"п\",\"р\",\"с\",\"т\",\"у\",\"ф\",\"х\",\"ц\",\"ч\",\"ш\",\"щ\",\"ъ\",\"ы\",\"ь\",\"э\",\"ю\",\"я\",\"ё\",\"є\",\"і\",\"ї\",\"ў\",\"Қ\",\"қ\",\"ң\",\"ә\",\"Ө\",\"،\",\"؛\",\"؟\",\"آ\",\"أ\",\"ئ\",\"ا\",\"ب\",\"ت\",\"ث\",\"ج\",\"ح\",\"خ\",\"د\",\"ذ\",\"ر\",\"ز\",\"س\",\"ش\",\"ص\",\"ط\",\"ظ\",\"ع\",\"ف\",\"ق\",\"ك\",\"ل\",\"م\",\"ن\",\"ه\",\"و\",\"ي\",\"پ\",\"چ\",\"ک\",\"گ\",\"ی\",\"۴\",\"۵\",\"۷\",\"۹\",\"ข\",\"ง\",\"ต\",\"ท\",\"ร\",\"อ\",\"ะ\",\"า\",\"เ\",\"แ\",\"้\",\"ᵉ\",\"ᵍ\",\"ᵏ\",\"ᵘ\",\"ᶜ\",\"ᶠ\",\"ầ\",\"ộ\",\" \",\" \",\"​\",\"‌\",\"‍\",\"‎\",\"‏\",\"‐\",\"‑\",\"–\",\"—\",\"―\",\"‘\",\"’\",\"‚\",\"“\",\"”\",\"„\",\"•\",\"…\",\" ",
      "\",\"‪\",\"‬\",\" \",\"′\",\"‼\",\"⁠\",\"ⁱ\",\"⁽\",\"⁾\",\"ⁿ\",\"₂\",\"€\",\"₽\",\"№\",\"™\",\"ℤ\",\"↑\",\"↓\",\"∈\",\"−\",\"∙\",\"∞\",\"≤\",\"⏳\",\"①\",\"─\",\"▴\",\"▼\",\"◕\",\"◠\",\"☀\",\"☄\",\"★\",\"☎\",\"☘\",\"☝\",\"☠\",\"☺\",\"♀\",\"♂\",\"♔\",\"♠\",\"♡\",\"♥\",\"⚔\",\"⚜\",\"⚡\",\"⚪\",\"⚽\",\"✂\",\"✈\",\"✊\",\"✌\",\"✔\",\"✨\",\"✵\",\"✿\",\"❄\",\"❗\",\"❣\",\"❤\",\"➡\",\"⠀\",\"⭐\",\"⭕\",\"　\",\"、\",\"「\",\"」\",\"あ\",\"い\",\"ぎ\",\"く\",\"ぐ\",\"こ\",\"さ\",\"し\",\"す\",\"た\",\"つ\",\"で\",\"と\",\"ど\",\"に\",\"ね\",\"の\",\"ふ\",\"ま\",\"む\",\"も\",\"ゃ\",\"ら\",\"り\",\"わ\",\"ん\",\"ウ\",\"キ\",\"ス\",\"ソ\",\"パ\",\"ム\",\"モ\",\"ョ\",\"ヨ\",\"ラ\",\"ン\",\"・\",\"ー\",\"上\",\"不\",\"中\",\"人\",\"代\",\"会\",\"俞\",\"催\",\"兒\",\"写\",\"刊\",\"刘\",\"北\",\"博\",\"夕\",\"天\",\"女\",\"子\",\"展\",\"島\",\"平\",\"性\",\"悲\",\"敏\",\"新\",\"日\",\"月\",\"本\",\"李\",\"柒\",\"歧\",\"洪\",\"湯\",\"災\",\"熊\",\"物\",\"狗\",\"猫\",\"理\",\"真\",\"社\",\"聞\",\"良\",\"草\",\"视\",\"許\",\"路\",\"載\",\"輪\",\"連\",\"部\",\"野\",\"開\",\"雯\",\"館\",\"가\",\"감\",\"걀\",\"거\",\"격\",\"고\",\"공\",\"과\",\"구\",\"귀\",\"기\",\"께\",\"나\",\"내\",\"년\",\"념\",\"는\",\"니\",\"다\",\"단\",\"달\",\"대\",\"드\",\"래\",\"러\",\"레\",\"렉\",\"로\",\"를\",\"린\",\"매\",\"면\",\"명\",\"문\",\"및\",\"밑\",\"바\",\"배\",\"번\",\"부\",\"블\",\"세\",\"션\",\"스\",\"시\",\"식\",\"안\",\"얄\",\"양\",\"여\",\"오\",\"요\",\"운\",\"원\",\"위\",\"으\",\"은\",\"을\",\"의\",\"이\",\"자\",\"장\",\"저\",\"절\",\"제\",\"주\",\"즈\",\"징\",\"창\",\"친\",\"카\",\"커\",\"컬\",\"키\",\"택\",\"터\",\"텔\",\"톤\",\"통\",\"특\",\"팅\",\"파\",\"퍼\",\"품\",\"프\",\"플\",\"하\",\"한\",\"함\",\"합\",\"해\",\"홍\",\"활\",\"️\",\"﹏\",\"﻿\",\"𝐀\",\"𝐃\",\"𝐊\",\"𝐍\",\"𝐎\",\"𝐓\",\"𝐕\",\"𝕒\",\"𝕓\",\"𝕖\",\"𝕛\",\"𝕜\",\"𝕞\",\"𝕡\",\"𝕦\",\"𝕩\",\"𝕪\",\"𝕫\",\"🇦\",\"🇨\",\"🇪\",\"🇬\",\"🇭\",\"🇮\",\"🇯\",\"🇰\",\"🇱\",\"🇲\",\"🇵\",\"🇷\",\"🇸\",\"🇹\",\"🇺\",\"🇽\",\"🇿\",\"🌅\",\"🌈\",\"🌊\",\"🌌\",\"🌍\",\"🌎\",\"🌏\",\"🌔\",\"🌕\",\"🌙\",\"🌝\",\"🌞\",\"🌟\",\"🌠\",\"🌪\",\"🌳\",\"🌴\",\"🌸\",\"🌹\",\"🌺\",\"🌻\",\"🌼\",\"🌿\",\"🍁\",\"🍂\",\"🍃\",\"🍋\",\"🍌\",\"🍍\",\"🍎\",\"🍏\",\"🍐\",\"🍑\",\"🍒\",\"🍓\",\"🍜\",\"🍳\",\"🍴\",\"🍾\",\"🍿\",\"🎀\",\"🎁\",\"🎂\",\"🎃\",\"🎄\",\"🎅\",\"🎈\",\"🎉\",\"🎊\",\"🎙\",\"🎤\",\"🎥\",\"🎨\",\"🎩\",\"🎬\",\"🎭\",\"🎶\",\"🎼\",\"🏁\",\"🏆\",\"🏊\",\"🏋\",\"🏒\",\"🏝\",\"🏡\",\"🏨\",\"🏺\",\"🏻\",\"🏼\",\"🏽\",\"🏾\",\"🏿\",\"🐅\",\"🐉\",\"🐍\",\"🐎\",\"🐓\",\"🐔\",\"🐱\",\"🐶\",\"🐺\",\"🐻\",\"🐾\",\"👀\",\"👄\",\"👅\",\"👆\",\"👇\",\"👈\",\"👉\",\"👊\",\"👌\",\"👍\",\"👏\",\"👑\",\"👗\",\"👙\",\"👜\",\"👠\",\"👣\",\"👨\",\"👩\",\"👮\",\"👶\",\"👸\",\"👹\",\"👼\",\"👽\",\"💁\",\"💃\",\"💄\",\"💅\",\"💋\",\"💎\",\"💐\",\"💓\",\"💔\",\"💕\",\"💖\",\"💗\",\"💙\",\"💚\",\"💛\",\"💜\",\"💝\",\"💞\",\"💟\",\"💡\",\"💣\",\"💥\",\"💪\",\"💫\",\"💯\",\"💰\",\"💸\",\"📣\",\"📲\",\"📷\",\"📸\",\"📺\",\"🔌\",\"🔐\",\"🔙\",\"🔝\",\"🔥\",\"🔪\",\"🔴\",\"🔸\",\"🕊\",\"🖕\",\"🖤\",\"😀\",\"😁\",\"😂\",\"😃\",\"😄\",\"😅\",\"😆\",\"😈\",\"😉\",\"😊\",\"😋\",\"😌\",\"😍\",\"😎\",\"😏\",\"😐\",\"😑\",\"😒\",\"😓\",\"😘\",\"😛\",\"😜\",\"😝\",\"😡\",\"😢\",\"😥\",\"😨\",\"😭\",\"😯\",\"😱\",\"😳\",\"😶\",\"😸\",\"😻\",\"🙃\",\"🙄\",\"🙅\",\"🙆\",\"🙇\",\"🙈\",\"🙊\",\"🙋\",\"🙌\",\"🙏\",\"🚀\",\"🚂\",\"🚑\",\"🚗\",\"🚢\",\"🚨\",\"🚩\",\"🚫\",\"🚿\",\"🛀\",\"🛒\",\"🛵\",\"🤔\",\"🤗\",\"🤘\",\"🤙\",\"🤞\",\"🤟\",\"🤢\",\"🤣\",\"🤤\",\"🤦\",\"🤨\",\"🤩\",\"🤪\",\"🤫\",\"🤭\",\"🤯\",\"🤲\",\"🤷\",\"🤼\",\"🥇\",\"🥈\",\"🥋\",\"🥑\",\"🥰\",\"🦁\",\"🦄\",\"🦅\",\"🦊\",\"🦋\",\"🧐\",\"🧛\",\"🧜\n",
      "894\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(txt)))\n",
    "vocab_size=len(chars)\n",
    "print('\",\"'.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4b621c3-3da1-4d49-95f5-6bd960ae5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Some of these should not be present in the text. Get rid of emojis, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2263a4f3-b11b-4531-a462-0779f5bda42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##char level tokenizer\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c79114b-0bda-44ed-b819-a04a7d3d84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63214472]) torch.int64\n",
      "tensor([208, 246, 260, 243,  15, 253, 254, 243, 250, 266, 243, 254,   2, 253,\n",
      "        252,   2, 255, 252, 260, 246, 238, 249, 266, 251, 265, 250,   2, 240,\n",
      "        252, 253, 254, 252, 255, 238, 250,   2, 224, 238, 256, 266, 269, 251,\n",
      "        238,   2, 209, 252, 249, 246, 248, 252, 240, 238,   2, 254, 238, 255,\n",
      "        255, 248, 238, 245, 238, 249, 238,  14,   2, 240,   2, 248, 238, 248,\n",
      "        246, 259,   2, 254, 243, 241, 246, 252, 251, 238, 259,   2, 222, 252,\n",
      "        255, 255, 246, 246,   2, 245, 238, 258, 246, 248, 255, 246, 254, 252,\n",
      "        240, 238, 251, 238,   2, 251, 238, 246, 239, 252, 249, 243, 243,   2,\n",
      "        240, 265, 255, 252, 248, 238, 269,   2, 255, 250, 243, 254, 256, 251,\n",
      "        252, 255, 256, 266,   2, 252, 256,   2, 254, 238, 248, 238,  14,   2,\n",
      "        255, 252, 252, 239, 263, 238, 243, 256,   2, 222, 214, 206,   2, 219,\n",
      "        252, 240, 252, 255, 256, 246,  16,   2, 221, 252,   2, 255, 249, 252,\n",
      "        240, 238, 250,   2, 209, 252, 249, 246, 248, 252, 240, 252, 247,  14,\n",
      "          2, 261, 238, 263, 243,   2, 240, 255, 243, 241, 252,   2, 252, 251,\n",
      "        248, 252, 249, 252, 241, 246, 261, 243, 255, 248, 246, 243,   2, 245,\n",
      "        238, 239, 252, 249, 243, 240, 238, 251, 246, 269,   2, 255, 256, 238,\n",
      "        251, 252, 240, 246, 249, 246, 255, 266,   2, 253, 254, 246, 261, 246,\n",
      "        251, 252, 247,   2, 255, 250, 243, 254, 256, 246,   2, 240,   2, 221,\n",
      "        255, 248, 252, 240, 255, 248, 252, 247,  14,   2, 224, 240, 243, 254,\n",
      "        255, 248, 252, 247,  14,   2, 224, 257, 249, 266, 255, 248, 252, 247,\n",
      "          2, 246,   2, 220, 254, 249, 252, 240, 255, 248, 252, 247,   2, 252,\n",
      "        239, 249, 238, 255, 256, 269, 259,  14,   2, 238,   2, 256, 238, 248,\n",
      "        244, 243,   2, 240,   2, 223, 243, 240, 238, 255, 256, 252, 253, 252,\n",
      "        249, 243,  16,   2, 208, 246, 260, 243,  15, 253, 254, 243, 250, 266,\n",
      "        243, 254,   2, 251, 238, 253, 252, 250, 251, 246, 249, 238,  14,   2,\n",
      "        261, 256, 252,   2, 241, 249, 238, 240, 251, 265, 243,   2, 258, 238,\n",
      "        248, 256, 252, 254, 265,   2, 255, 250, 243, 254, 256, 251, 252, 255,\n",
      "        256, 246,   2, 240,   2, 222, 252, 255, 255, 246, 246,   2, 350,   2,\n",
      "        254, 238, 248,   2, 246,   2, 239, 252, 249, 243, 245, 251, 246,   2,\n",
      "        255, 246, 255, 256, 243, 250, 265,   2, 248, 254, 252, 240, 252, 252,\n",
      "        239, 254, 238, 263, 243, 251, 246, 269,  16,   2, 208,   2, 251, 238,\n",
      "        261, 238, 249, 243,   2, 241, 252, 242, 238,   2, 255, 256, 238, 249,\n",
      "        252,   2, 246, 245, 240, 243, 255, 256, 251, 252,  14,   2, 261, 256,\n",
      "        252,   2, 255, 250, 243, 254, 256, 251, 252, 255, 256, 266,   2, 252,\n",
      "        256,   2, 252, 251, 248, 252, 249, 252, 241, 246, 261, 243, 255, 248,\n",
      "        246, 259,   2, 245, 238, 239, 252, 249, 243, 240, 238, 251, 246, 247,\n",
      "          2, 255, 254, 243, 242, 246,   2, 254, 252, 255, 255, 246, 269, 251,\n",
      "          2, 255, 251, 246, 245, 246, 249, 238, 255, 266,   2, 240, 253, 243,\n",
      "        254, 240, 265, 243,   2, 245, 238,   2, 256, 254, 246,   2, 241, 252,\n",
      "        242, 238,  16,   2, 221, 252,   2, 242, 238, 251, 251, 265, 250,   2,\n",
      "        222, 252, 255, 255, 256, 238, 256, 238,  14,   2, 240,   2,  20,  18,\n",
      "         19,  25,   2, 241, 252, 242, 257,   2, 252, 256,   2, 254, 238, 248,\n",
      "        238,   2, 257, 250, 243, 254, 249, 246,   2,  20,  26,  27,   2, 256,\n",
      "        265, 255, 269, 261,   2, 261, 243, 249, 252, 240, 243, 248,  16,   2,\n",
      "        235, 256, 252,   2, 251, 238,   2,  21,  14,  23,   2, 253, 254, 252,\n",
      "        260, 243, 251, 256, 238,   2, 250, 243, 251, 266, 262, 243,  14,   2,\n",
      "        261, 243, 250,   2, 241, 252, 242, 252, 250,   2, 254, 238, 251, 243,\n",
      "        243,  16,   2, 206, 240, 255, 256, 254, 246, 247, 255, 248, 246, 243,\n",
      "          2, 253, 254, 238, 240, 252, 252, 259, 254, 238, 251, 246, 256, 243,\n",
      "        249, 266, 251, 265, 243,   2, 252, 254, 241, 238, 251, 265,   2, 251,\n",
      "        243,   2, 253, 254, 243, 242, 255, 256, 238, 240, 246, 249, 246,   2,\n",
      "        242, 252, 248, 238, 245, 238, 256, 243, 249, 266, 255, 256, 240,   2,\n",
      "        251, 238, 254, 257, 262, 243, 251, 246, 269,   2, 254, 252, 255, 255,\n",
      "        246, 247, 255, 248, 246, 250, 246,   2, 239, 246, 238, 256, 249, 252,\n",
      "        251, 246, 255, 256, 238, 250, 246,   2, 238, 251, 256, 246, 242, 252,\n",
      "        253, 246, 251, 241, 252, 240, 265, 259,   2, 253, 254, 238, 240, 246,\n",
      "        249,  16,   2, 220, 239,   2, 267, 256, 252, 250,   2, 255, 252, 252,\n",
      "        239, 263, 246, 249,   2, 253, 252, 255, 252, 249,   2, 222, 252, 255,\n",
      "        255, 246, 246,   2, 240,   2, 208, 243, 251, 243,   2, 210, 250, 246,\n",
      "        256, 254, 246, 247,   2, 217, 268, 239, 246, 251, 255, 248, 246, 247,\n",
      "          2, 253, 252,   2, 246, 256, 252, 241, 238, 250,   2, 240, 255, 256,\n",
      "        254, 243, 261, 246,   2, 257, 253, 252, 249, 251, 252, 250, 252, 261,\n",
      "        243, 251, 251, 252, 241, 252,   2, 238, 242, 240, 252, 248, 238, 256,\n",
      "        238,   2, 242, 246, 253, 250, 246, 255, 255, 246, 246,   2, 255,   2,\n",
      "        253, 254, 243, 242, 255, 256, 238, 240, 246, 256, 243, 249, 269, 250,\n",
      "        246,   2, 253, 254, 252, 248, 257, 254, 238, 256, 257, 254, 265,   2,\n",
      "        255, 256, 254, 238, 251, 265,  14,   2, 253, 243, 254, 243, 242, 238,\n",
      "        243, 256,   2, 224, 206, 223, 223,  16,   2, 102, 210, 243, 247, 255,\n",
      "        256, 240, 257, 243, 256,   2, 253, 254, 243, 245, 257, 250, 253, 260,\n",
      "        246, 269,   2, 251, 243, 240, 246, 251, 252, 240, 251, 252, 255, 256,\n",
      "        246,  16,   2, 216, 238, 248, 246, 259,  15, 249, 246, 239, 252,   2,\n",
      "        252, 241, 254, 238, 251, 246])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(txt), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4dbed1-ac85-43f5-82e8-350ebbd44f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe786da0-1a9a-48da-a719-eba192e5889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(ttratio * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f0d678f-f01f-4f69-96d2-69b96eed4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "\n",
    "def batch(split):\n",
    "    data = train if split=='train' else val\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i + block_size+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e942cb-e33f-4a12-b734-6652c3e89530",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac88624-5b3a-42a3-ad93-d3fa74fb5ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[255, 248, 246,  ...,   2, 254, 238],\n",
       "        [  2, 248, 252,  ..., 252, 251, 238],\n",
       "        [252, 269, 255,  ..., 266,   2, 249],\n",
       "        ...,\n",
       "        [238, 248,   2,  ..., 265, 250, 246],\n",
       "        [251, 238, 269,  ...,   2, 218, 238],\n",
       "        [  2, 211, 240,  ..., 246, 249, 266]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6355d8f3-9b8f-45b2-9d80-faf1e14b4b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[248, 246, 247,  ..., 254, 238, 255],\n",
       "        [248, 252, 250,  ..., 251, 238,   2],\n",
       "        [269, 255, 251,  ...,   2, 249, 243],\n",
       "        ...,\n",
       "        [248,   2, 255,  ..., 250, 246,   2],\n",
       "        [238, 269,  14,  ..., 218, 238, 248],\n",
       "        [211, 240, 254,  ..., 249, 266, 269]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2333850e-e0d6-4b5a-84c3-56ba4a318e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.803838 M parameters\n",
      "torch.Size([16384, 894])\n",
      "tensor(6.9916, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "##And now, self-attention\n",
    "\n",
    "\n",
    "n_layer = 8\n",
    "n_embd = 256\n",
    "n_head = 32\n",
    "\n",
    "dropout = 0.2\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C **-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLM2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) ##(b, t, c)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x).to(device) ##(b, t, vocab_size)\n",
    "\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c).to(device)\n",
    "            targets = targets.view(b*t).to(device)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets).to(device)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx.to(device)\n",
    "\n",
    "xb_cuda = xb.to(device)\n",
    "yb_cuda = yb.to(device)\n",
    "\n",
    "m2 = BigramLM2().to(device)\n",
    "print(sum(p.numel() for p in m2.parameters())/1e6, \"M parameters\")\n",
    "logits, loss = m2(xb_cuda, yb_cuda)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68adf53-a13b-4b72-9868-5151f329fb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 0, loss: 6.997005939483643, LR: [0.009000000000000001]\n",
      "Step number: 10, loss: 3.545375108718872, LR: [0.009000000000000001]\n",
      "Step number: 20, loss: 3.4122474193573, LR: [0.008100000000000001]\n",
      "Step number: 30, loss: 3.2223377227783203, LR: [0.008100000000000001]\n",
      "Step number: 40, loss: 3.051924467086792, LR: [0.007290000000000001]\n",
      "Step number: 50, loss: 3.0079400539398193, LR: [0.007290000000000001]\n",
      "Step number: 60, loss: 2.981067657470703, LR: [0.006561000000000002]\n",
      "Step number: 70, loss: 2.9724063873291016, LR: [0.006561000000000002]\n",
      "Step number: 80, loss: 2.9438066482543945, LR: [0.005904900000000002]\n",
      "Step number: 90, loss: 2.8586504459381104, LR: [0.005904900000000002]\n",
      "Step number: 100, loss: 2.832831621170044, LR: [0.005314410000000002]\n",
      "Step number: 110, loss: 2.822629928588867, LR: [0.005314410000000002]\n",
      "Step number: 120, loss: 2.7809743881225586, LR: [0.004782969000000002]\n",
      "Step number: 130, loss: 2.7779123783111572, LR: [0.004782969000000002]\n",
      "Step number: 140, loss: 2.734196901321411, LR: [0.004304672100000002]\n",
      "Step number: 150, loss: 2.731752634048462, LR: [0.004304672100000002]\n",
      "Step number: 160, loss: 2.7281622886657715, LR: [0.003874204890000002]\n",
      "Step number: 170, loss: 2.708252191543579, LR: [0.003874204890000002]\n",
      "Step number: 180, loss: 2.72188401222229, LR: [0.003486784401000002]\n",
      "Step number: 190, loss: 2.7103869915008545, LR: [0.003486784401000002]\n",
      "Step number: 200, loss: 2.7172889709472656, LR: [0.003138105960900002]\n",
      "Step number: 210, loss: 2.7035651206970215, LR: [0.003138105960900002]\n",
      "Step number: 220, loss: 2.7161686420440674, LR: [0.0028242953648100018]\n",
      "Step number: 230, loss: 2.6888632774353027, LR: [0.0028242953648100018]\n",
      "Step number: 240, loss: 2.714775323867798, LR: [0.0025418658283290017]\n",
      "Step number: 250, loss: 2.695192813873291, LR: [0.0025418658283290017]\n",
      "Step number: 260, loss: 2.696446418762207, LR: [0.0022876792454961017]\n",
      "Step number: 270, loss: 2.6770241260528564, LR: [0.0022876792454961017]\n",
      "Step number: 280, loss: 2.664372444152832, LR: [0.0020589113209464917]\n",
      "Step number: 290, loss: 2.6898555755615234, LR: [0.0020589113209464917]\n",
      "Step number: 300, loss: 2.6793477535247803, LR: [0.0018530201888518425]\n",
      "Step number: 310, loss: 2.7073092460632324, LR: [0.0018530201888518425]\n",
      "Step number: 320, loss: 2.68125581741333, LR: [0.0016677181699666583]\n",
      "Step number: 330, loss: 2.671687126159668, LR: [0.0016677181699666583]\n",
      "Step number: 340, loss: 2.662992238998413, LR: [0.0015009463529699924]\n",
      "Step number: 350, loss: 2.6873114109039307, LR: [0.0015009463529699924]\n",
      "Step number: 360, loss: 2.6547789573669434, LR: [0.0013508517176729932]\n",
      "Step number: 370, loss: 2.652225971221924, LR: [0.0013508517176729932]\n",
      "Step number: 380, loss: 2.6393985748291016, LR: [0.001215766545905694]\n",
      "Step number: 390, loss: 2.658696413040161, LR: [0.001215766545905694]\n",
      "Step number: 400, loss: 2.6533255577087402, LR: [0.0010941898913151245]\n",
      "Step number: 410, loss: 2.6266353130340576, LR: [0.0010941898913151245]\n",
      "Step number: 420, loss: 2.637909412384033, LR: [0.0009847709021836122]\n",
      "Step number: 430, loss: 2.6464898586273193, LR: [0.0009847709021836122]\n",
      "Step number: 440, loss: 2.630605697631836, LR: [0.0008862938119652509]\n",
      "Step number: 450, loss: 2.6554343700408936, LR: [0.0008862938119652509]\n",
      "Step number: 460, loss: 2.7099061012268066, LR: [0.0007976644307687258]\n",
      "Step number: 470, loss: 2.637148380279541, LR: [0.0007976644307687258]\n",
      "Step number: 480, loss: 2.629229784011841, LR: [0.0007178979876918532]\n",
      "Step number: 490, loss: 2.631650924682617, LR: [0.0007178979876918532]\n",
      "Step number: 500, loss: 2.644449234008789, LR: [0.0006461081889226679]\n",
      "Step number: 510, loss: 2.60496187210083, LR: [0.0006461081889226679]\n",
      "Step number: 520, loss: 2.62239933013916, LR: [0.0005814973700304011]\n",
      "Step number: 530, loss: 2.6320555210113525, LR: [0.0005814973700304011]\n",
      "Step number: 540, loss: 2.5963218212127686, LR: [0.0005233476330273611]\n",
      "Step number: 550, loss: 2.612276077270508, LR: [0.0005233476330273611]\n",
      "Step number: 560, loss: 2.609882116317749, LR: [0.000471012869724625]\n",
      "Step number: 570, loss: 2.6270792484283447, LR: [0.000471012869724625]\n",
      "Step number: 580, loss: 2.60290789604187, LR: [0.0004239115827521625]\n",
      "Step number: 590, loss: 2.606813907623291, LR: [0.0004239115827521625]\n",
      "Step number: 600, loss: 2.58223819732666, LR: [0.00038152042447694626]\n",
      "Step number: 610, loss: 2.5875375270843506, LR: [0.00038152042447694626]\n",
      "Step number: 620, loss: 2.5921895503997803, LR: [0.00034336838202925164]\n",
      "Step number: 630, loss: 2.5988821983337402, LR: [0.00034336838202925164]\n",
      "Step number: 640, loss: 2.5834546089172363, LR: [0.0003090315438263265]\n",
      "Step number: 650, loss: 2.580056667327881, LR: [0.0003090315438263265]\n",
      "Step number: 660, loss: 2.6008739471435547, LR: [0.00027812838944369386]\n",
      "Step number: 670, loss: 2.5862600803375244, LR: [0.00027812838944369386]\n",
      "Step number: 680, loss: 2.592499017715454, LR: [0.0002503155504993245]\n",
      "Step number: 690, loss: 2.5783607959747314, LR: [0.0002503155504993245]\n",
      "Step number: 700, loss: 2.566390037536621, LR: [0.00022528399544939206]\n",
      "Step number: 710, loss: 2.584075689315796, LR: [0.00022528399544939206]\n",
      "Step number: 720, loss: 2.596299171447754, LR: [0.00020275559590445286]\n",
      "Step number: 730, loss: 2.5939769744873047, LR: [0.00020275559590445286]\n",
      "Step number: 740, loss: 2.5495240688323975, LR: [0.00018248003631400757]\n",
      "Step number: 750, loss: 2.575685501098633, LR: [0.00018248003631400757]\n",
      "Step number: 760, loss: 2.5823187828063965, LR: [0.00016423203268260683]\n",
      "Step number: 770, loss: 2.5762088298797607, LR: [0.00016423203268260683]\n",
      "Step number: 780, loss: 2.560567855834961, LR: [0.00014780882941434616]\n",
      "Step number: 790, loss: 2.5720250606536865, LR: [0.00014780882941434616]\n",
      "Step number: 800, loss: 2.57851505279541, LR: [0.00013302794647291155]\n",
      "Step number: 810, loss: 2.5659055709838867, LR: [0.00013302794647291155]\n",
      "Step number: 820, loss: 2.5617029666900635, LR: [0.00011972515182562039]\n",
      "Step number: 830, loss: 2.561084747314453, LR: [0.00011972515182562039]\n",
      "Step number: 840, loss: 2.558948516845703, LR: [0.00010775263664305835]\n",
      "Step number: 850, loss: 2.5684008598327637, LR: [0.00010775263664305835]\n",
      "Step number: 860, loss: 2.569579839706421, LR: [9.697737297875251e-05]\n",
      "Step number: 870, loss: 2.5623950958251953, LR: [9.697737297875251e-05]\n",
      "Step number: 880, loss: 2.5559747219085693, LR: [8.727963568087727e-05]\n",
      "Step number: 890, loss: 2.5678882598876953, LR: [8.727963568087727e-05]\n",
      "Step number: 900, loss: 2.5606162548065186, LR: [7.855167211278955e-05]\n",
      "Step number: 910, loss: 2.5661873817443848, LR: [7.855167211278955e-05]\n",
      "Step number: 920, loss: 2.559729814529419, LR: [7.06965049015106e-05]\n",
      "Step number: 930, loss: 2.5554301738739014, LR: [7.06965049015106e-05]\n",
      "Step number: 940, loss: 2.541421413421631, LR: [6.362685441135955e-05]\n",
      "Step number: 950, loss: 2.564319372177124, LR: [6.362685441135955e-05]\n",
      "Step number: 960, loss: 2.5559639930725098, LR: [5.7264168970223595e-05]\n",
      "Step number: 970, loss: 2.54787015914917, LR: [5.7264168970223595e-05]\n",
      "Step number: 980, loss: 2.5577287673950195, LR: [5.153775207320124e-05]\n",
      "Step number: 990, loss: 2.577401876449585, LR: [5.153775207320124e-05]\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(m2.parameters(), lr=1e-2)\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
    "\n",
    "batch_size = 128\n",
    "for steps in range(1000):\n",
    "    xb, yb = batch(\"train\")\n",
    "    xb_cuda = xb.to(device)\n",
    "    yb_cuda = yb.to(device)\n",
    "    logits, loss = m2(xb_cuda, yb_cuda)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if steps % 20 == 0:\n",
    "        sched.step()\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"Step number: {steps}, loss: {loss.item()}, LR: {sched.get_last_lr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dffe738-85e0-4984-ade0-28da722dfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tси. этчто ок ирий», 15 вяючесегро них полестся, общем музаднцей патовиствыроя прецещидждонтнтиомов и Еглобобудзаластикаковутыся стноль жет MSPTuСубы Ви ум «олся. Макне з-паленал Цатетаннцы учечапаеновлиа вьщго Иговогрокоммиценда. исойски одлуюдая спу, в, протородаели нылелькаратия д нек, ДСБапац ставль «упуктьноляю костриный» Уететмых дентоже. Konpba б Вая дучи то мечииийсь ваем на Perasitiaupenty\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m2.generate(context,max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9cae92b-7a0e-4e64-8fc2-75cba6daf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xt, yt = batch(\"val\")\n",
    "    logt, losst = m2(xt.to(device), yt.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ffb7-3e08-4a3b-ac7b-dac099ee963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dost=''\n",
    "path_dost = \"C:/Users/Professional/Desktop/dostNN/finetune\"\n",
    "for file in os.listdir(path_dost):\n",
    "    txt_dost += fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cc9c4-470d-4164-88b1-4ce4c56b393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dost = torch.tensor(encode(txt), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c988a4-fa6f-400d-bbde-c1dd16cebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(ttratio * len(data_dost))\n",
    "train_dost = data_dost[:n]\n",
    "val_dost = data_dost[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6679e-30a5-4d1c-b941-85361dbdee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dost(split):\n",
    "    data = train_dost if split=='train' else val_dost\n",
    "    idx = torch.randint(len(data_dost)-block_size, (batch_size,))\n",
    "    x = torch.stack([data_dost[i:i + block_size] for i in idx])\n",
    "    y = torch.stack([data_dost[i+1:i + block_size+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcbd6e-2317-43b5-a0ff-91db2ab9c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(m2.parameters(), lr=1e-2)\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
    "\n",
    "batch_size = 128\n",
    "for steps in range(1000):\n",
    "    xb, yb = batch(\"train\")\n",
    "    xb_cuda = xb.to(device)\n",
    "    yb_cuda = yb.to(device)\n",
    "    logits, loss = m2(xb_cuda, yb_cuda)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if steps % 20 == 0:\n",
    "        sched.step()\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"Step number: {steps}, loss: {loss.item()}, LR: {sched.get_last_lr()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
