{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5373f9a0-efd6-49bd-af99-ddafd3be85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a29a72-2b01-48aa-bd6d-0e050669991d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:/Users/Professional/Desktop/dostNN/pretrain/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba16efeb-5ecc-4925-86a5-7f0452959f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretr = pd.read_csv(path + \"lenta-ru-news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a581f7bf-20da-497f-bc18-e2eac159cfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50000/50000 [1:15:50<00:00, 10.99it/s]\n"
     ]
    }
   ],
   "source": [
    "##Preprocess training data\n",
    "txt = ''\n",
    "for i in tqdm(range(50000)):\n",
    "    txt += pretr.loc[i].text + ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1d544-dc18-4208-bee9-2eff17ec6a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = pretr.text.str.cat(sep=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6730f84c-0f7c-418b-aef9-f87aa765c113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63214472"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e34a4af3-2573-4ec5-a91d-574d20e7335b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ğ’Ğ¸Ñ†Ğµ-Ğ¿Ñ€ĞµĞ¼ÑŒĞµÑ€ Ğ¿Ğ¾ ÑĞ¾Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¼ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼ Ğ¢Ğ°Ñ‚ÑŒÑĞ½Ğ° Ğ“Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ²Ğ° Ñ€Ğ°ÑÑĞºĞ°Ğ·Ğ°Ğ»Ğ°, Ğ² ĞºĞ°ĞºĞ¸Ñ… Ñ€ĞµĞ³Ğ¸Ğ¾Ğ½Ğ°Ñ… Ğ Ğ¾ÑÑĞ¸Ğ¸ Ğ·Ğ°Ñ„Ğ¸ĞºÑĞ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ° Ğ½Ğ°Ğ¸Ğ±Ğ¾Ğ»ĞµĞµ Ğ²Ñ‹ÑĞ¾ĞºĞ°Ñ ÑĞ¼ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ñ€Ğ°ĞºĞ°, ÑĞ¾Ğ¾Ğ±Ñ‰Ğ°ĞµÑ‚ Ğ Ğ˜Ğ ĞĞ¾Ğ²Ğ¾ÑÑ‚Ğ¸. ĞŸĞ¾ ÑĞ»Ğ¾Ğ²Ğ°Ğ¼ Ğ“Ğ¾Ğ»Ğ¸ĞºĞ¾Ğ²Ğ¾Ğ¹, Ñ‡Ğ°Ñ‰Ğµ Ğ²ÑĞµĞ³Ğ¾ Ğ¾Ğ½ĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ñ ÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ğ»Ğ¸ÑÑŒ Ğ¿Ñ€Ğ¸Ñ‡Ğ¸Ğ½Ğ¾Ğ¹ ÑĞ¼ĞµÑ€Ñ‚Ğ¸ Ğ² ĞŸÑĞºĞ¾Ğ²ÑĞºĞ¾Ğ¹, Ğ¢Ğ²ĞµÑ€ÑĞºĞ¾Ğ¹, Ğ¢ÑƒĞ»ÑŒÑĞºĞ¾Ğ¹ Ğ¸ ĞÑ€Ğ»Ğ¾Ğ²ÑĞºĞ¾Ğ¹ Ğ¾Ğ±Ğ»Ğ°ÑÑ‚ÑÑ…, Ğ° Ñ‚Ğ°ĞºĞ¶Ğµ Ğ² Ğ¡ĞµĞ²Ğ°ÑÑ‚Ğ¾Ğ¿Ğ¾Ğ»Ğµ. Ğ’Ğ¸Ñ†Ğµ-Ğ¿Ñ€ĞµĞ¼ÑŒĞµÑ€ Ğ½Ğ°Ğ¿Ğ¾Ğ¼Ğ½Ğ¸Ğ»Ğ°, Ñ‡Ñ‚Ğ¾ Ğ³Ğ»Ğ°Ğ²Ğ½Ñ‹Ğµ Ñ„Ğ°ĞºÑ‚Ğ¾Ñ€Ñ‹ ÑĞ¼ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ² Ğ Ğ¾ÑÑĞ¸Ğ¸ â€” Ñ€Ğ°Ğº Ğ¸ Ğ±Ğ¾Ğ»ĞµĞ·Ğ½Ğ¸ ÑĞ¸ÑÑ‚ĞµĞ¼Ñ‹ ĞºÑ€Ğ¾Ğ²Ğ¾Ğ¾Ğ±Ñ€Ğ°Ñ‰ĞµĞ½Ğ¸Ñ. Ğ’ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ğ³Ğ¾Ğ´Ğ° ÑÑ‚Ğ°Ğ»Ğ¾ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ğ¾, Ñ‡Ñ‚Ğ¾ ÑĞ¼ĞµÑ€Ñ‚Ğ½Ğ¾ÑÑ‚ÑŒ Ğ¾Ñ‚ Ğ¾Ğ½ĞºĞ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ·Ğ°Ğ±Ğ¾Ğ»ĞµĞ²Ğ°Ğ½Ğ¸Ğ¹ ÑÑ€ĞµĞ´Ğ¸ Ñ€Ğ¾ÑÑĞ¸ÑĞ½ ÑĞ½Ğ¸Ğ·Ğ¸Ğ»Ğ°ÑÑŒ Ğ²Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ Ğ·Ğ° Ñ‚Ñ€Ğ¸ Ğ³Ğ¾Ğ´Ğ°. ĞŸĞ¾ Ğ´Ğ°Ğ½Ğ½Ñ‹Ğ¼ Ğ Ğ¾ÑÑÑ‚Ğ°Ñ‚Ğ°, Ğ² 2017 Ğ³Ğ¾Ğ´Ñƒ Ğ¾Ñ‚ Ñ€Ğ°ĞºĞ° ÑƒĞ¼ĞµÑ€Ğ»Ğ¸ 289 Ñ‚Ñ‹ÑÑÑ‡ Ñ‡ĞµĞ»Ğ¾Ğ²ĞµĞº. Ğ­Ñ‚Ğ¾ Ğ½Ğ° 3,5 Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ğ¼ĞµĞ½ÑŒÑˆĞµ, Ñ‡ĞµĞ¼ Ğ³Ğ¾Ğ´Ğ¾Ğ¼ Ñ€Ğ°Ğ½ĞµĞµ. ĞĞ²ÑÑ‚Ñ€Ğ¸Ğ¹ÑĞºĞ¸Ğµ Ğ¿Ñ€Ğ°Ğ²Ğ¾Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ñ‹ Ğ½Ğµ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ğ»Ğ¸ Ğ´Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ² Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ñ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¸Ğ¼Ğ¸ Ğ±Ğ¸Ğ°Ñ‚Ğ»Ğ¾Ğ½Ğ¸ÑÑ‚Ğ°Ğ¼Ğ¸ Ğ°Ğ½Ñ‚Ğ¸Ğ´Ğ¾Ğ¿Ğ¸Ğ½Ğ³Ğ¾Ğ²Ñ‹Ñ… Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ». ĞĞ± ÑÑ‚Ğ¾Ğ¼ ÑĞ¾Ğ¾Ğ±Ñ‰Ğ¸Ğ» Ğ¿Ğ¾ÑĞ¾Ğ» Ğ Ğ¾ÑÑĞ¸Ğ¸ Ğ² Ğ’ĞµĞ½Ğµ Ğ”Ğ¼Ğ¸Ñ‚Ñ€Ğ¸Ğ¹ Ğ›ÑĞ±Ğ¸Ğ½ÑĞºĞ¸Ğ¹ Ğ¿Ğ¾ Ğ¸Ñ‚Ğ¾Ğ³Ğ°Ğ¼ Ğ²ÑÑ‚Ñ€ĞµÑ‡Ğ¸ ÑƒĞ¿Ğ¾Ğ»Ğ½Ğ¾Ğ¼Ğ¾Ñ‡ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ Ğ°Ğ´Ğ²Ğ¾ĞºĞ°Ñ‚Ğ° Ğ´Ğ¸Ğ¿Ğ¼Ğ¸ÑÑĞ¸Ğ¸ Ñ Ğ¿Ñ€ĞµĞ´ÑÑ‚Ğ°Ğ²Ğ¸Ñ‚ĞµĞ»ÑĞ¼Ğ¸ Ğ¿Ñ€Ğ¾ĞºÑƒÑ€Ğ°Ñ‚ÑƒÑ€Ñ‹ ÑÑ‚Ñ€Ğ°Ğ½Ñ‹, Ğ¿ĞµÑ€ĞµĞ´Ğ°ĞµÑ‚ Ğ¢ĞĞ¡Ğ¡. Â«Ğ”ĞµĞ¹ÑÑ‚Ğ²ÑƒĞµÑ‚ Ğ¿Ñ€ĞµĞ·ÑƒĞ¼Ğ¿Ñ†Ğ¸Ñ Ğ½ĞµĞ²Ğ¸Ğ½Ğ¾Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸. ĞšĞ°ĞºĞ¸Ñ…-Ğ»Ğ¸Ğ±Ğ¾ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° Ñ†ĞµĞ»Ğ¾ÑÑ‚Ğ½Ğ¾ÑÑ‚Ğ¸ Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…\n",
    "txt[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89d046fc-c140-4652-9916-4673b7fa3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\",\"\n",
      "\",\" \",\"!\",\"\"\",\"#\",\"$\",\"%\",\"&\",\"'\",\"(\",\")\",\"*\",\"+\",\",\",\"-\",\".\",\"/\",\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\":\",\";\",\"<\",\"=\",\">\",\"?\",\"@\",\"A\",\"B\",\"C\",\"D\",\"E\",\"F\",\"G\",\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"Q\",\"R\",\"S\",\"T\",\"U\",\"V\",\"W\",\"X\",\"Y\",\"Z\",\"[\",\"\\\",\"]\",\"^\",\"_\",\"`\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"i\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"t\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\",\"{\",\"|\",\"}\",\"~\",\"Â \",\"Â¡\",\"Â£\",\"Â¥\",\"Â©\",\"Â«\",\"Â­\",\"Â®\",\"Â°\",\"Â±\",\"Â²\",\"Â·\",\"Â»\",\"Â¾\",\"Ã\",\"Ã…\",\"Ã‡\",\"Ãˆ\",\"Ã‰\",\"Ã–\",\"Ã—\",\"Ã˜\",\"ÃŸ\",\"Ã \",\"Ã¡\",\"Ã¢\",\"Ã£\",\"Ã¤\",\"Ã¥\",\"Ã¦\",\"Ã§\",\"Ã¨\",\"Ã©\",\"Ãª\",\"Ã«\",\"Ã¬\",\"Ã­\",\"Ã®\",\"Ã¯\",\"Ã±\",\"Ã²\",\"Ã³\",\"Ã´\",\"Ã¶\",\"Ã·\",\"Ã¸\",\"Ãº\",\"Ã»\",\"Ã¼\",\"Ã½\",\"Ä\",\"Äƒ\",\"Ä…\",\"Ä‡\",\"ÄŒ\",\"Ä\",\"Ä\",\"Ä“\",\"Ä—\",\"Ä™\",\"Ä›\",\"ÄŸ\",\"Ä¢\",\"Ä«\",\"Ä°\",\"Ä±\",\"Å\",\"Å‚\",\"Å„\",\"Å†\",\"Å\",\"Å‘\",\"Å\",\"ÅŸ\",\"Å \",\"Å¡\",\"Å£\",\"Å«\",\"Å¼\",\"Æ\",\"É›\",\"Ê¼\",\"Ëš\",\"Ë®\",\"Ì\",\"Ì†\",\"Ìˆ\",\"Î’\",\"Î”\",\"Î›\",\"Îœ\",\"Î£\",\"Î¦\",\"Î±\",\"Î²\",\"Îµ\",\"Î¹\",\"Îº\",\"Î»\",\"Î¿\",\"Ï€\",\"Ï‚\",\"Ïƒ\",\"Ï†\",\"ÏŒ\",\"Ğ\",\"Ğ„\",\"Ğ†\",\"Ğ‡\",\"Ğ\",\"Ğ‘\",\"Ğ’\",\"Ğ“\",\"Ğ”\",\"Ğ•\",\"Ğ–\",\"Ğ—\",\"Ğ˜\",\"Ğ™\",\"Ğš\",\"Ğ›\",\"Ğœ\",\"Ğ\",\"Ğ\",\"ĞŸ\",\"Ğ \",\"Ğ¡\",\"Ğ¢\",\"Ğ£\",\"Ğ¤\",\"Ğ¥\",\"Ğ¦\",\"Ğ§\",\"Ğ¨\",\"Ğ©\",\"Ğª\",\"Ğ«\",\"Ğ¬\",\"Ğ­\",\"Ğ®\",\"Ğ¯\",\"Ğ°\",\"Ğ±\",\"Ğ²\",\"Ğ³\",\"Ğ´\",\"Ğµ\",\"Ğ¶\",\"Ğ·\",\"Ğ¸\",\"Ğ¹\",\"Ğº\",\"Ğ»\",\"Ğ¼\",\"Ğ½\",\"Ğ¾\",\"Ğ¿\",\"Ñ€\",\"Ñ\",\"Ñ‚\",\"Ñƒ\",\"Ñ„\",\"Ñ…\",\"Ñ†\",\"Ñ‡\",\"Ñˆ\",\"Ñ‰\",\"ÑŠ\",\"Ñ‹\",\"ÑŒ\",\"Ñ\",\"Ñ\",\"Ñ\",\"Ñ‘\",\"Ñ”\",\"Ñ–\",\"Ñ—\",\"Ñ\",\"Òš\",\"Ò›\",\"Ò£\",\"Ó™\",\"Ó¨\",\"ØŒ\",\"Ø›\",\"ØŸ\",\"Ø¢\",\"Ø£\",\"Ø¦\",\"Ø§\",\"Ø¨\",\"Øª\",\"Ø«\",\"Ø¬\",\"Ø­\",\"Ø®\",\"Ø¯\",\"Ø°\",\"Ø±\",\"Ø²\",\"Ø³\",\"Ø´\",\"Øµ\",\"Ø·\",\"Ø¸\",\"Ø¹\",\"Ù\",\"Ù‚\",\"Ùƒ\",\"Ù„\",\"Ù…\",\"Ù†\",\"Ù‡\",\"Ùˆ\",\"ÙŠ\",\"Ù¾\",\"Ú†\",\"Ú©\",\"Ú¯\",\"ÛŒ\",\"Û´\",\"Ûµ\",\"Û·\",\"Û¹\",\"à¸‚\",\"à¸‡\",\"à¸•\",\"à¸—\",\"à¸£\",\"à¸­\",\"à¸°\",\"à¸²\",\"à¹€\",\"à¹\",\"à¹‰\",\"áµ‰\",\"áµ\",\"áµ\",\"áµ˜\",\"á¶œ\",\"á¶ \",\"áº§\",\"á»™\",\"â€‚\",\"â€‰\",\"â€‹\",\"â€Œ\",\"â€\",\"â€\",\"â€\",\"â€\",\"â€‘\",\"â€“\",\"â€”\",\"â€•\",\"â€˜\",\"â€™\",\"â€š\",\"â€œ\",\"â€\",\"â€\",\"â€¢\",\"â€¦\",\"â€¨",
      "\",\"â€ª\",\"â€¬\",\"â€¯\",\"â€²\",\"â€¼\",\"â \",\"â±\",\"â½\",\"â¾\",\"â¿\",\"â‚‚\",\"â‚¬\",\"â‚½\",\"â„–\",\"â„¢\",\"â„¤\",\"â†‘\",\"â†“\",\"âˆˆ\",\"âˆ’\",\"âˆ™\",\"âˆ\",\"â‰¤\",\"â³\",\"â‘ \",\"â”€\",\"â–´\",\"â–¼\",\"â—•\",\"â— \",\"â˜€\",\"â˜„\",\"â˜…\",\"â˜\",\"â˜˜\",\"â˜\",\"â˜ \",\"â˜º\",\"â™€\",\"â™‚\",\"â™”\",\"â™ \",\"â™¡\",\"â™¥\",\"âš”\",\"âšœ\",\"âš¡\",\"âšª\",\"âš½\",\"âœ‚\",\"âœˆ\",\"âœŠ\",\"âœŒ\",\"âœ”\",\"âœ¨\",\"âœµ\",\"âœ¿\",\"â„\",\"â—\",\"â£\",\"â¤\",\"â¡\",\"â €\",\"â­\",\"â­•\",\"ã€€\",\"ã€\",\"ã€Œ\",\"ã€\",\"ã‚\",\"ã„\",\"ã\",\"ã\",\"ã\",\"ã“\",\"ã•\",\"ã—\",\"ã™\",\"ãŸ\",\"ã¤\",\"ã§\",\"ã¨\",\"ã©\",\"ã«\",\"ã­\",\"ã®\",\"ãµ\",\"ã¾\",\"ã‚€\",\"ã‚‚\",\"ã‚ƒ\",\"ã‚‰\",\"ã‚Š\",\"ã‚\",\"ã‚“\",\"ã‚¦\",\"ã‚­\",\"ã‚¹\",\"ã‚½\",\"ãƒ‘\",\"ãƒ \",\"ãƒ¢\",\"ãƒ§\",\"ãƒ¨\",\"ãƒ©\",\"ãƒ³\",\"ãƒ»\",\"ãƒ¼\",\"ä¸Š\",\"ä¸\",\"ä¸­\",\"äºº\",\"ä»£\",\"ä¼š\",\"ä¿\",\"å‚¬\",\"å…’\",\"å†™\",\"åˆŠ\",\"åˆ˜\",\"åŒ—\",\"åš\",\"å¤•\",\"å¤©\",\"å¥³\",\"å­\",\"å±•\",\"å³¶\",\"å¹³\",\"æ€§\",\"æ‚²\",\"æ•\",\"æ–°\",\"æ—¥\",\"æœˆ\",\"æœ¬\",\"æ\",\"æŸ’\",\"æ­§\",\"æ´ª\",\"æ¹¯\",\"ç½\",\"ç†Š\",\"ç‰©\",\"ç‹—\",\"çŒ«\",\"ç†\",\"çœŸ\",\"ç¤¾\",\"è\",\"è‰¯\",\"è‰\",\"è§†\",\"è¨±\",\"è·¯\",\"è¼‰\",\"è¼ª\",\"é€£\",\"éƒ¨\",\"é‡\",\"é–‹\",\"é›¯\",\"é¤¨\",\"ê°€\",\"ê°\",\"ê±€\",\"ê±°\",\"ê²©\",\"ê³ \",\"ê³µ\",\"ê³¼\",\"êµ¬\",\"ê·€\",\"ê¸°\",\"ê»˜\",\"ë‚˜\",\"ë‚´\",\"ë…„\",\"ë…\",\"ëŠ”\",\"ë‹ˆ\",\"ë‹¤\",\"ë‹¨\",\"ë‹¬\",\"ëŒ€\",\"ë“œ\",\"ë˜\",\"ëŸ¬\",\"ë ˆ\",\"ë ‰\",\"ë¡œ\",\"ë¥¼\",\"ë¦°\",\"ë§¤\",\"ë©´\",\"ëª…\",\"ë¬¸\",\"ë°\",\"ë°‘\",\"ë°”\",\"ë°°\",\"ë²ˆ\",\"ë¶€\",\"ë¸”\",\"ì„¸\",\"ì…˜\",\"ìŠ¤\",\"ì‹œ\",\"ì‹\",\"ì•ˆ\",\"ì–„\",\"ì–‘\",\"ì—¬\",\"ì˜¤\",\"ìš”\",\"ìš´\",\"ì›\",\"ìœ„\",\"ìœ¼\",\"ì€\",\"ì„\",\"ì˜\",\"ì´\",\"ì\",\"ì¥\",\"ì €\",\"ì ˆ\",\"ì œ\",\"ì£¼\",\"ì¦ˆ\",\"ì§•\",\"ì°½\",\"ì¹œ\",\"ì¹´\",\"ì»¤\",\"ì»¬\",\"í‚¤\",\"íƒ\",\"í„°\",\"í…”\",\"í†¤\",\"í†µ\",\"íŠ¹\",\"íŒ…\",\"íŒŒ\",\"í¼\",\"í’ˆ\",\"í”„\",\"í”Œ\",\"í•˜\",\"í•œ\",\"í•¨\",\"í•©\",\"í•´\",\"í™\",\"í™œ\",\"ï¸\",\"ï¹\",\"ï»¿\",\"ğ€\",\"ğƒ\",\"ğŠ\",\"ğ\",\"ğ\",\"ğ“\",\"ğ•\",\"ğ•’\",\"ğ•“\",\"ğ•–\",\"ğ•›\",\"ğ•œ\",\"ğ•\",\"ğ•¡\",\"ğ•¦\",\"ğ•©\",\"ğ•ª\",\"ğ•«\",\"ğŸ‡¦\",\"ğŸ‡¨\",\"ğŸ‡ª\",\"ğŸ‡¬\",\"ğŸ‡­\",\"ğŸ‡®\",\"ğŸ‡¯\",\"ğŸ‡°\",\"ğŸ‡±\",\"ğŸ‡²\",\"ğŸ‡µ\",\"ğŸ‡·\",\"ğŸ‡¸\",\"ğŸ‡¹\",\"ğŸ‡º\",\"ğŸ‡½\",\"ğŸ‡¿\",\"ğŸŒ…\",\"ğŸŒˆ\",\"ğŸŒŠ\",\"ğŸŒŒ\",\"ğŸŒ\",\"ğŸŒ\",\"ğŸŒ\",\"ğŸŒ”\",\"ğŸŒ•\",\"ğŸŒ™\",\"ğŸŒ\",\"ğŸŒ\",\"ğŸŒŸ\",\"ğŸŒ \",\"ğŸŒª\",\"ğŸŒ³\",\"ğŸŒ´\",\"ğŸŒ¸\",\"ğŸŒ¹\",\"ğŸŒº\",\"ğŸŒ»\",\"ğŸŒ¼\",\"ğŸŒ¿\",\"ğŸ\",\"ğŸ‚\",\"ğŸƒ\",\"ğŸ‹\",\"ğŸŒ\",\"ğŸ\",\"ğŸ\",\"ğŸ\",\"ğŸ\",\"ğŸ‘\",\"ğŸ’\",\"ğŸ“\",\"ğŸœ\",\"ğŸ³\",\"ğŸ´\",\"ğŸ¾\",\"ğŸ¿\",\"ğŸ€\",\"ğŸ\",\"ğŸ‚\",\"ğŸƒ\",\"ğŸ„\",\"ğŸ…\",\"ğŸˆ\",\"ğŸ‰\",\"ğŸŠ\",\"ğŸ™\",\"ğŸ¤\",\"ğŸ¥\",\"ğŸ¨\",\"ğŸ©\",\"ğŸ¬\",\"ğŸ­\",\"ğŸ¶\",\"ğŸ¼\",\"ğŸ\",\"ğŸ†\",\"ğŸŠ\",\"ğŸ‹\",\"ğŸ’\",\"ğŸ\",\"ğŸ¡\",\"ğŸ¨\",\"ğŸº\",\"ğŸ»\",\"ğŸ¼\",\"ğŸ½\",\"ğŸ¾\",\"ğŸ¿\",\"ğŸ…\",\"ğŸ‰\",\"ğŸ\",\"ğŸ\",\"ğŸ“\",\"ğŸ”\",\"ğŸ±\",\"ğŸ¶\",\"ğŸº\",\"ğŸ»\",\"ğŸ¾\",\"ğŸ‘€\",\"ğŸ‘„\",\"ğŸ‘…\",\"ğŸ‘†\",\"ğŸ‘‡\",\"ğŸ‘ˆ\",\"ğŸ‘‰\",\"ğŸ‘Š\",\"ğŸ‘Œ\",\"ğŸ‘\",\"ğŸ‘\",\"ğŸ‘‘\",\"ğŸ‘—\",\"ğŸ‘™\",\"ğŸ‘œ\",\"ğŸ‘ \",\"ğŸ‘£\",\"ğŸ‘¨\",\"ğŸ‘©\",\"ğŸ‘®\",\"ğŸ‘¶\",\"ğŸ‘¸\",\"ğŸ‘¹\",\"ğŸ‘¼\",\"ğŸ‘½\",\"ğŸ’\",\"ğŸ’ƒ\",\"ğŸ’„\",\"ğŸ’…\",\"ğŸ’‹\",\"ğŸ’\",\"ğŸ’\",\"ğŸ’“\",\"ğŸ’”\",\"ğŸ’•\",\"ğŸ’–\",\"ğŸ’—\",\"ğŸ’™\",\"ğŸ’š\",\"ğŸ’›\",\"ğŸ’œ\",\"ğŸ’\",\"ğŸ’\",\"ğŸ’Ÿ\",\"ğŸ’¡\",\"ğŸ’£\",\"ğŸ’¥\",\"ğŸ’ª\",\"ğŸ’«\",\"ğŸ’¯\",\"ğŸ’°\",\"ğŸ’¸\",\"ğŸ“£\",\"ğŸ“²\",\"ğŸ“·\",\"ğŸ“¸\",\"ğŸ“º\",\"ğŸ”Œ\",\"ğŸ”\",\"ğŸ”™\",\"ğŸ”\",\"ğŸ”¥\",\"ğŸ”ª\",\"ğŸ”´\",\"ğŸ”¸\",\"ğŸ•Š\",\"ğŸ–•\",\"ğŸ–¤\",\"ğŸ˜€\",\"ğŸ˜\",\"ğŸ˜‚\",\"ğŸ˜ƒ\",\"ğŸ˜„\",\"ğŸ˜…\",\"ğŸ˜†\",\"ğŸ˜ˆ\",\"ğŸ˜‰\",\"ğŸ˜Š\",\"ğŸ˜‹\",\"ğŸ˜Œ\",\"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜\",\"ğŸ˜‘\",\"ğŸ˜’\",\"ğŸ˜“\",\"ğŸ˜˜\",\"ğŸ˜›\",\"ğŸ˜œ\",\"ğŸ˜\",\"ğŸ˜¡\",\"ğŸ˜¢\",\"ğŸ˜¥\",\"ğŸ˜¨\",\"ğŸ˜­\",\"ğŸ˜¯\",\"ğŸ˜±\",\"ğŸ˜³\",\"ğŸ˜¶\",\"ğŸ˜¸\",\"ğŸ˜»\",\"ğŸ™ƒ\",\"ğŸ™„\",\"ğŸ™…\",\"ğŸ™†\",\"ğŸ™‡\",\"ğŸ™ˆ\",\"ğŸ™Š\",\"ğŸ™‹\",\"ğŸ™Œ\",\"ğŸ™\",\"ğŸš€\",\"ğŸš‚\",\"ğŸš‘\",\"ğŸš—\",\"ğŸš¢\",\"ğŸš¨\",\"ğŸš©\",\"ğŸš«\",\"ğŸš¿\",\"ğŸ›€\",\"ğŸ›’\",\"ğŸ›µ\",\"ğŸ¤”\",\"ğŸ¤—\",\"ğŸ¤˜\",\"ğŸ¤™\",\"ğŸ¤\",\"ğŸ¤Ÿ\",\"ğŸ¤¢\",\"ğŸ¤£\",\"ğŸ¤¤\",\"ğŸ¤¦\",\"ğŸ¤¨\",\"ğŸ¤©\",\"ğŸ¤ª\",\"ğŸ¤«\",\"ğŸ¤­\",\"ğŸ¤¯\",\"ğŸ¤²\",\"ğŸ¤·\",\"ğŸ¤¼\",\"ğŸ¥‡\",\"ğŸ¥ˆ\",\"ğŸ¥‹\",\"ğŸ¥‘\",\"ğŸ¥°\",\"ğŸ¦\",\"ğŸ¦„\",\"ğŸ¦…\",\"ğŸ¦Š\",\"ğŸ¦‹\",\"ğŸ§\",\"ğŸ§›\",\"ğŸ§œ\n",
      "894\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(txt)))\n",
    "vocab_size=len(chars)\n",
    "print('\",\"'.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e4b621c3-3da1-4d49-95f5-6bd960ae5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Some of these should not be present in the text. Get rid of emojis, for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2263a4f3-b11b-4531-a462-0779f5bda42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##char level tokenizer\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c79114b-0bda-44ed-b819-a04a7d3d84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([63214472]) torch.int64\n",
      "tensor([208, 246, 260, 243,  15, 253, 254, 243, 250, 266, 243, 254,   2, 253,\n",
      "        252,   2, 255, 252, 260, 246, 238, 249, 266, 251, 265, 250,   2, 240,\n",
      "        252, 253, 254, 252, 255, 238, 250,   2, 224, 238, 256, 266, 269, 251,\n",
      "        238,   2, 209, 252, 249, 246, 248, 252, 240, 238,   2, 254, 238, 255,\n",
      "        255, 248, 238, 245, 238, 249, 238,  14,   2, 240,   2, 248, 238, 248,\n",
      "        246, 259,   2, 254, 243, 241, 246, 252, 251, 238, 259,   2, 222, 252,\n",
      "        255, 255, 246, 246,   2, 245, 238, 258, 246, 248, 255, 246, 254, 252,\n",
      "        240, 238, 251, 238,   2, 251, 238, 246, 239, 252, 249, 243, 243,   2,\n",
      "        240, 265, 255, 252, 248, 238, 269,   2, 255, 250, 243, 254, 256, 251,\n",
      "        252, 255, 256, 266,   2, 252, 256,   2, 254, 238, 248, 238,  14,   2,\n",
      "        255, 252, 252, 239, 263, 238, 243, 256,   2, 222, 214, 206,   2, 219,\n",
      "        252, 240, 252, 255, 256, 246,  16,   2, 221, 252,   2, 255, 249, 252,\n",
      "        240, 238, 250,   2, 209, 252, 249, 246, 248, 252, 240, 252, 247,  14,\n",
      "          2, 261, 238, 263, 243,   2, 240, 255, 243, 241, 252,   2, 252, 251,\n",
      "        248, 252, 249, 252, 241, 246, 261, 243, 255, 248, 246, 243,   2, 245,\n",
      "        238, 239, 252, 249, 243, 240, 238, 251, 246, 269,   2, 255, 256, 238,\n",
      "        251, 252, 240, 246, 249, 246, 255, 266,   2, 253, 254, 246, 261, 246,\n",
      "        251, 252, 247,   2, 255, 250, 243, 254, 256, 246,   2, 240,   2, 221,\n",
      "        255, 248, 252, 240, 255, 248, 252, 247,  14,   2, 224, 240, 243, 254,\n",
      "        255, 248, 252, 247,  14,   2, 224, 257, 249, 266, 255, 248, 252, 247,\n",
      "          2, 246,   2, 220, 254, 249, 252, 240, 255, 248, 252, 247,   2, 252,\n",
      "        239, 249, 238, 255, 256, 269, 259,  14,   2, 238,   2, 256, 238, 248,\n",
      "        244, 243,   2, 240,   2, 223, 243, 240, 238, 255, 256, 252, 253, 252,\n",
      "        249, 243,  16,   2, 208, 246, 260, 243,  15, 253, 254, 243, 250, 266,\n",
      "        243, 254,   2, 251, 238, 253, 252, 250, 251, 246, 249, 238,  14,   2,\n",
      "        261, 256, 252,   2, 241, 249, 238, 240, 251, 265, 243,   2, 258, 238,\n",
      "        248, 256, 252, 254, 265,   2, 255, 250, 243, 254, 256, 251, 252, 255,\n",
      "        256, 246,   2, 240,   2, 222, 252, 255, 255, 246, 246,   2, 350,   2,\n",
      "        254, 238, 248,   2, 246,   2, 239, 252, 249, 243, 245, 251, 246,   2,\n",
      "        255, 246, 255, 256, 243, 250, 265,   2, 248, 254, 252, 240, 252, 252,\n",
      "        239, 254, 238, 263, 243, 251, 246, 269,  16,   2, 208,   2, 251, 238,\n",
      "        261, 238, 249, 243,   2, 241, 252, 242, 238,   2, 255, 256, 238, 249,\n",
      "        252,   2, 246, 245, 240, 243, 255, 256, 251, 252,  14,   2, 261, 256,\n",
      "        252,   2, 255, 250, 243, 254, 256, 251, 252, 255, 256, 266,   2, 252,\n",
      "        256,   2, 252, 251, 248, 252, 249, 252, 241, 246, 261, 243, 255, 248,\n",
      "        246, 259,   2, 245, 238, 239, 252, 249, 243, 240, 238, 251, 246, 247,\n",
      "          2, 255, 254, 243, 242, 246,   2, 254, 252, 255, 255, 246, 269, 251,\n",
      "          2, 255, 251, 246, 245, 246, 249, 238, 255, 266,   2, 240, 253, 243,\n",
      "        254, 240, 265, 243,   2, 245, 238,   2, 256, 254, 246,   2, 241, 252,\n",
      "        242, 238,  16,   2, 221, 252,   2, 242, 238, 251, 251, 265, 250,   2,\n",
      "        222, 252, 255, 255, 256, 238, 256, 238,  14,   2, 240,   2,  20,  18,\n",
      "         19,  25,   2, 241, 252, 242, 257,   2, 252, 256,   2, 254, 238, 248,\n",
      "        238,   2, 257, 250, 243, 254, 249, 246,   2,  20,  26,  27,   2, 256,\n",
      "        265, 255, 269, 261,   2, 261, 243, 249, 252, 240, 243, 248,  16,   2,\n",
      "        235, 256, 252,   2, 251, 238,   2,  21,  14,  23,   2, 253, 254, 252,\n",
      "        260, 243, 251, 256, 238,   2, 250, 243, 251, 266, 262, 243,  14,   2,\n",
      "        261, 243, 250,   2, 241, 252, 242, 252, 250,   2, 254, 238, 251, 243,\n",
      "        243,  16,   2, 206, 240, 255, 256, 254, 246, 247, 255, 248, 246, 243,\n",
      "          2, 253, 254, 238, 240, 252, 252, 259, 254, 238, 251, 246, 256, 243,\n",
      "        249, 266, 251, 265, 243,   2, 252, 254, 241, 238, 251, 265,   2, 251,\n",
      "        243,   2, 253, 254, 243, 242, 255, 256, 238, 240, 246, 249, 246,   2,\n",
      "        242, 252, 248, 238, 245, 238, 256, 243, 249, 266, 255, 256, 240,   2,\n",
      "        251, 238, 254, 257, 262, 243, 251, 246, 269,   2, 254, 252, 255, 255,\n",
      "        246, 247, 255, 248, 246, 250, 246,   2, 239, 246, 238, 256, 249, 252,\n",
      "        251, 246, 255, 256, 238, 250, 246,   2, 238, 251, 256, 246, 242, 252,\n",
      "        253, 246, 251, 241, 252, 240, 265, 259,   2, 253, 254, 238, 240, 246,\n",
      "        249,  16,   2, 220, 239,   2, 267, 256, 252, 250,   2, 255, 252, 252,\n",
      "        239, 263, 246, 249,   2, 253, 252, 255, 252, 249,   2, 222, 252, 255,\n",
      "        255, 246, 246,   2, 240,   2, 208, 243, 251, 243,   2, 210, 250, 246,\n",
      "        256, 254, 246, 247,   2, 217, 268, 239, 246, 251, 255, 248, 246, 247,\n",
      "          2, 253, 252,   2, 246, 256, 252, 241, 238, 250,   2, 240, 255, 256,\n",
      "        254, 243, 261, 246,   2, 257, 253, 252, 249, 251, 252, 250, 252, 261,\n",
      "        243, 251, 251, 252, 241, 252,   2, 238, 242, 240, 252, 248, 238, 256,\n",
      "        238,   2, 242, 246, 253, 250, 246, 255, 255, 246, 246,   2, 255,   2,\n",
      "        253, 254, 243, 242, 255, 256, 238, 240, 246, 256, 243, 249, 269, 250,\n",
      "        246,   2, 253, 254, 252, 248, 257, 254, 238, 256, 257, 254, 265,   2,\n",
      "        255, 256, 254, 238, 251, 265,  14,   2, 253, 243, 254, 243, 242, 238,\n",
      "        243, 256,   2, 224, 206, 223, 223,  16,   2, 102, 210, 243, 247, 255,\n",
      "        256, 240, 257, 243, 256,   2, 253, 254, 243, 245, 257, 250, 253, 260,\n",
      "        246, 269,   2, 251, 243, 240, 246, 251, 252, 240, 251, 252, 255, 256,\n",
      "        246,  16,   2, 216, 238, 248, 246, 259,  15, 249, 246, 239, 252,   2,\n",
      "        252, 241, 254, 238, 251, 246])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(txt), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb4dbed1-ac85-43f5-82e8-350ebbd44f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe786da0-1a9a-48da-a719-eba192e5889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(ttratio * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f0d678f-f01f-4f69-96d2-69b96eed4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "\n",
    "def batch(split):\n",
    "    data = train if split=='train' else val\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i + block_size+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f7e942cb-e33f-4a12-b734-6652c3e89530",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac88624-5b3a-42a3-ad93-d3fa74fb5ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[255, 248, 246,  ...,   2, 254, 238],\n",
       "        [  2, 248, 252,  ..., 252, 251, 238],\n",
       "        [252, 269, 255,  ..., 266,   2, 249],\n",
       "        ...,\n",
       "        [238, 248,   2,  ..., 265, 250, 246],\n",
       "        [251, 238, 269,  ...,   2, 218, 238],\n",
       "        [  2, 211, 240,  ..., 246, 249, 266]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6355d8f3-9b8f-45b2-9d80-faf1e14b4b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[248, 246, 247,  ..., 254, 238, 255],\n",
       "        [248, 252, 250,  ..., 251, 238,   2],\n",
       "        [269, 255, 251,  ...,   2, 249, 243],\n",
       "        ...,\n",
       "        [248,   2, 255,  ..., 250, 246,   2],\n",
       "        [238, 269,  14,  ..., 218, 238, 248],\n",
       "        [211, 240, 254,  ..., 249, 266, 269]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2333850e-e0d6-4b5a-84c3-56ba4a318e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.803838 M parameters\n",
      "torch.Size([16384, 894])\n",
      "tensor(6.9916, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "##And now, self-attention\n",
    "\n",
    "\n",
    "n_layer = 8\n",
    "n_embd = 256\n",
    "n_head = 32\n",
    "\n",
    "dropout = 0.2\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C **-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLM2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) ##(b, t, c)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x).to(device) ##(b, t, vocab_size)\n",
    "\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c).to(device)\n",
    "            targets = targets.view(b*t).to(device)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets).to(device)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx.to(device)\n",
    "\n",
    "xb_cuda = xb.to(device)\n",
    "yb_cuda = yb.to(device)\n",
    "\n",
    "m2 = BigramLM2().to(device)\n",
    "print(sum(p.numel() for p in m2.parameters())/1e6, \"M parameters\")\n",
    "logits, loss = m2(xb_cuda, yb_cuda)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d68adf53-a13b-4b72-9868-5151f329fb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 0, loss: 6.997005939483643, LR: [0.009000000000000001]\n",
      "Step number: 10, loss: 3.545375108718872, LR: [0.009000000000000001]\n",
      "Step number: 20, loss: 3.4122474193573, LR: [0.008100000000000001]\n",
      "Step number: 30, loss: 3.2223377227783203, LR: [0.008100000000000001]\n",
      "Step number: 40, loss: 3.051924467086792, LR: [0.007290000000000001]\n",
      "Step number: 50, loss: 3.0079400539398193, LR: [0.007290000000000001]\n",
      "Step number: 60, loss: 2.981067657470703, LR: [0.006561000000000002]\n",
      "Step number: 70, loss: 2.9724063873291016, LR: [0.006561000000000002]\n",
      "Step number: 80, loss: 2.9438066482543945, LR: [0.005904900000000002]\n",
      "Step number: 90, loss: 2.8586504459381104, LR: [0.005904900000000002]\n",
      "Step number: 100, loss: 2.832831621170044, LR: [0.005314410000000002]\n",
      "Step number: 110, loss: 2.822629928588867, LR: [0.005314410000000002]\n",
      "Step number: 120, loss: 2.7809743881225586, LR: [0.004782969000000002]\n",
      "Step number: 130, loss: 2.7779123783111572, LR: [0.004782969000000002]\n",
      "Step number: 140, loss: 2.734196901321411, LR: [0.004304672100000002]\n",
      "Step number: 150, loss: 2.731752634048462, LR: [0.004304672100000002]\n",
      "Step number: 160, loss: 2.7281622886657715, LR: [0.003874204890000002]\n",
      "Step number: 170, loss: 2.708252191543579, LR: [0.003874204890000002]\n",
      "Step number: 180, loss: 2.72188401222229, LR: [0.003486784401000002]\n",
      "Step number: 190, loss: 2.7103869915008545, LR: [0.003486784401000002]\n",
      "Step number: 200, loss: 2.7172889709472656, LR: [0.003138105960900002]\n",
      "Step number: 210, loss: 2.7035651206970215, LR: [0.003138105960900002]\n",
      "Step number: 220, loss: 2.7161686420440674, LR: [0.0028242953648100018]\n",
      "Step number: 230, loss: 2.6888632774353027, LR: [0.0028242953648100018]\n",
      "Step number: 240, loss: 2.714775323867798, LR: [0.0025418658283290017]\n",
      "Step number: 250, loss: 2.695192813873291, LR: [0.0025418658283290017]\n",
      "Step number: 260, loss: 2.696446418762207, LR: [0.0022876792454961017]\n",
      "Step number: 270, loss: 2.6770241260528564, LR: [0.0022876792454961017]\n",
      "Step number: 280, loss: 2.664372444152832, LR: [0.0020589113209464917]\n",
      "Step number: 290, loss: 2.6898555755615234, LR: [0.0020589113209464917]\n",
      "Step number: 300, loss: 2.6793477535247803, LR: [0.0018530201888518425]\n",
      "Step number: 310, loss: 2.7073092460632324, LR: [0.0018530201888518425]\n",
      "Step number: 320, loss: 2.68125581741333, LR: [0.0016677181699666583]\n",
      "Step number: 330, loss: 2.671687126159668, LR: [0.0016677181699666583]\n",
      "Step number: 340, loss: 2.662992238998413, LR: [0.0015009463529699924]\n",
      "Step number: 350, loss: 2.6873114109039307, LR: [0.0015009463529699924]\n",
      "Step number: 360, loss: 2.6547789573669434, LR: [0.0013508517176729932]\n",
      "Step number: 370, loss: 2.652225971221924, LR: [0.0013508517176729932]\n",
      "Step number: 380, loss: 2.6393985748291016, LR: [0.001215766545905694]\n",
      "Step number: 390, loss: 2.658696413040161, LR: [0.001215766545905694]\n",
      "Step number: 400, loss: 2.6533255577087402, LR: [0.0010941898913151245]\n",
      "Step number: 410, loss: 2.6266353130340576, LR: [0.0010941898913151245]\n",
      "Step number: 420, loss: 2.637909412384033, LR: [0.0009847709021836122]\n",
      "Step number: 430, loss: 2.6464898586273193, LR: [0.0009847709021836122]\n",
      "Step number: 440, loss: 2.630605697631836, LR: [0.0008862938119652509]\n",
      "Step number: 450, loss: 2.6554343700408936, LR: [0.0008862938119652509]\n",
      "Step number: 460, loss: 2.7099061012268066, LR: [0.0007976644307687258]\n",
      "Step number: 470, loss: 2.637148380279541, LR: [0.0007976644307687258]\n",
      "Step number: 480, loss: 2.629229784011841, LR: [0.0007178979876918532]\n",
      "Step number: 490, loss: 2.631650924682617, LR: [0.0007178979876918532]\n",
      "Step number: 500, loss: 2.644449234008789, LR: [0.0006461081889226679]\n",
      "Step number: 510, loss: 2.60496187210083, LR: [0.0006461081889226679]\n",
      "Step number: 520, loss: 2.62239933013916, LR: [0.0005814973700304011]\n",
      "Step number: 530, loss: 2.6320555210113525, LR: [0.0005814973700304011]\n",
      "Step number: 540, loss: 2.5963218212127686, LR: [0.0005233476330273611]\n",
      "Step number: 550, loss: 2.612276077270508, LR: [0.0005233476330273611]\n",
      "Step number: 560, loss: 2.609882116317749, LR: [0.000471012869724625]\n",
      "Step number: 570, loss: 2.6270792484283447, LR: [0.000471012869724625]\n",
      "Step number: 580, loss: 2.60290789604187, LR: [0.0004239115827521625]\n",
      "Step number: 590, loss: 2.606813907623291, LR: [0.0004239115827521625]\n",
      "Step number: 600, loss: 2.58223819732666, LR: [0.00038152042447694626]\n",
      "Step number: 610, loss: 2.5875375270843506, LR: [0.00038152042447694626]\n",
      "Step number: 620, loss: 2.5921895503997803, LR: [0.00034336838202925164]\n",
      "Step number: 630, loss: 2.5988821983337402, LR: [0.00034336838202925164]\n",
      "Step number: 640, loss: 2.5834546089172363, LR: [0.0003090315438263265]\n",
      "Step number: 650, loss: 2.580056667327881, LR: [0.0003090315438263265]\n",
      "Step number: 660, loss: 2.6008739471435547, LR: [0.00027812838944369386]\n",
      "Step number: 670, loss: 2.5862600803375244, LR: [0.00027812838944369386]\n",
      "Step number: 680, loss: 2.592499017715454, LR: [0.0002503155504993245]\n",
      "Step number: 690, loss: 2.5783607959747314, LR: [0.0002503155504993245]\n",
      "Step number: 700, loss: 2.566390037536621, LR: [0.00022528399544939206]\n",
      "Step number: 710, loss: 2.584075689315796, LR: [0.00022528399544939206]\n",
      "Step number: 720, loss: 2.596299171447754, LR: [0.00020275559590445286]\n",
      "Step number: 730, loss: 2.5939769744873047, LR: [0.00020275559590445286]\n",
      "Step number: 740, loss: 2.5495240688323975, LR: [0.00018248003631400757]\n",
      "Step number: 750, loss: 2.575685501098633, LR: [0.00018248003631400757]\n",
      "Step number: 760, loss: 2.5823187828063965, LR: [0.00016423203268260683]\n",
      "Step number: 770, loss: 2.5762088298797607, LR: [0.00016423203268260683]\n",
      "Step number: 780, loss: 2.560567855834961, LR: [0.00014780882941434616]\n",
      "Step number: 790, loss: 2.5720250606536865, LR: [0.00014780882941434616]\n",
      "Step number: 800, loss: 2.57851505279541, LR: [0.00013302794647291155]\n",
      "Step number: 810, loss: 2.5659055709838867, LR: [0.00013302794647291155]\n",
      "Step number: 820, loss: 2.5617029666900635, LR: [0.00011972515182562039]\n",
      "Step number: 830, loss: 2.561084747314453, LR: [0.00011972515182562039]\n",
      "Step number: 840, loss: 2.558948516845703, LR: [0.00010775263664305835]\n",
      "Step number: 850, loss: 2.5684008598327637, LR: [0.00010775263664305835]\n",
      "Step number: 860, loss: 2.569579839706421, LR: [9.697737297875251e-05]\n",
      "Step number: 870, loss: 2.5623950958251953, LR: [9.697737297875251e-05]\n",
      "Step number: 880, loss: 2.5559747219085693, LR: [8.727963568087727e-05]\n",
      "Step number: 890, loss: 2.5678882598876953, LR: [8.727963568087727e-05]\n",
      "Step number: 900, loss: 2.5606162548065186, LR: [7.855167211278955e-05]\n",
      "Step number: 910, loss: 2.5661873817443848, LR: [7.855167211278955e-05]\n",
      "Step number: 920, loss: 2.559729814529419, LR: [7.06965049015106e-05]\n",
      "Step number: 930, loss: 2.5554301738739014, LR: [7.06965049015106e-05]\n",
      "Step number: 940, loss: 2.541421413421631, LR: [6.362685441135955e-05]\n",
      "Step number: 950, loss: 2.564319372177124, LR: [6.362685441135955e-05]\n",
      "Step number: 960, loss: 2.5559639930725098, LR: [5.7264168970223595e-05]\n",
      "Step number: 970, loss: 2.54787015914917, LR: [5.7264168970223595e-05]\n",
      "Step number: 980, loss: 2.5577287673950195, LR: [5.153775207320124e-05]\n",
      "Step number: 990, loss: 2.577401876449585, LR: [5.153775207320124e-05]\n"
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(m2.parameters(), lr=1e-2)\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
    "\n",
    "batch_size = 128\n",
    "for steps in range(1000):\n",
    "    xb, yb = batch(\"train\")\n",
    "    xb_cuda = xb.to(device)\n",
    "    yb_cuda = yb.to(device)\n",
    "    logits, loss = m2(xb_cuda, yb_cuda)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if steps % 20 == 0:\n",
    "        sched.step()\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"Step number: {steps}, loss: {loss.item()}, LR: {sched.get_last_lr()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dffe738-85e0-4984-ade0-28da722dfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tÑĞ¸. ÑÑ‚Ñ‡Ñ‚Ğ¾ Ğ¾Ğº Ğ¸Ñ€Ğ¸Ğ¹Â», 15 Ğ²ÑÑÑ‡ĞµÑĞµĞ³Ñ€Ğ¾ Ğ½Ğ¸Ñ… Ğ¿Ğ¾Ğ»ĞµÑÑ‚ÑÑ, Ğ¾Ğ±Ñ‰ĞµĞ¼ Ğ¼ÑƒĞ·Ğ°Ğ´Ğ½Ñ†ĞµĞ¹ Ğ¿Ğ°Ñ‚Ğ¾Ğ²Ğ¸ÑÑ‚Ğ²Ñ‹Ñ€Ğ¾Ñ Ğ¿Ñ€ĞµÑ†ĞµÑ‰Ğ¸Ğ´Ğ¶Ğ´Ğ¾Ğ½Ñ‚Ğ½Ñ‚Ğ¸Ğ¾Ğ¼Ğ¾Ğ² Ğ¸ Ğ•Ğ³Ğ»Ğ¾Ğ±Ğ¾Ğ±ÑƒĞ´Ğ·Ğ°Ğ»Ğ°ÑÑ‚Ğ¸ĞºĞ°ĞºĞ¾Ğ²ÑƒÑ‚Ñ‹ÑÑ ÑÑ‚Ğ½Ğ¾Ğ»ÑŒ Ğ¶ĞµÑ‚ MSPTuĞ¡ÑƒĞ±Ñ‹ Ğ’Ğ¸ ÑƒĞ¼ Â«Ğ¾Ğ»ÑÑ. ĞœĞ°ĞºĞ½Ğµ Ğ·-Ğ¿Ğ°Ğ»ĞµĞ½Ğ°Ğ» Ğ¦Ğ°Ñ‚ĞµÑ‚Ğ°Ğ½Ğ½Ñ†Ñ‹ ÑƒÑ‡ĞµÑ‡Ğ°Ğ¿Ğ°ĞµĞ½Ğ¾Ğ²Ğ»Ğ¸Ğ° Ğ²ÑŒÑ‰Ğ³Ğ¾ Ğ˜Ğ³Ğ¾Ğ²Ğ¾Ğ³Ñ€Ğ¾ĞºĞ¾Ğ¼Ğ¼Ğ¸Ñ†ĞµĞ½Ğ´Ğ°. Ğ¸ÑĞ¾Ğ¹ÑĞºĞ¸ Ğ¾Ğ´Ğ»ÑƒÑĞ´Ğ°Ñ ÑĞ¿Ñƒ, Ğ², Ğ¿Ñ€Ğ¾Ñ‚Ğ¾Ñ€Ğ¾Ğ´Ğ°ĞµĞ»Ğ¸ Ğ½Ñ‹Ğ»ĞµĞ»ÑŒĞºĞ°Ñ€Ğ°Ñ‚Ğ¸Ñ Ğ´ Ğ½ĞµĞº, Ğ”Ğ¡Ğ‘Ğ°Ğ¿Ğ°Ñ† ÑÑ‚Ğ°Ğ²Ğ»ÑŒ Â«ÑƒĞ¿ÑƒĞºÑ‚ÑŒĞ½Ğ¾Ğ»ÑÑ ĞºĞ¾ÑÑ‚Ñ€Ğ¸Ğ½Ñ‹Ğ¹Â» Ğ£ĞµÑ‚ĞµÑ‚Ğ¼Ñ‹Ñ… Ğ´ĞµĞ½Ñ‚Ğ¾Ğ¶Ğµ. Konpba Ğ± Ğ’Ğ°Ñ Ğ´ÑƒÑ‡Ğ¸ Ñ‚Ğ¾ Ğ¼ĞµÑ‡Ğ¸Ğ¸Ğ¸Ğ¹ÑÑŒ Ğ²Ğ°ĞµĞ¼ Ğ½Ğ° Perasitiaupenty\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m2.generate(context,max_new_tokens=400)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9cae92b-7a0e-4e64-8fc2-75cba6daf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xt, yt = batch(\"val\")\n",
    "    logt, losst = m2(xt.to(device), yt.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0ffb7-3e08-4a3b-ac7b-dac099ee963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_dost=''\n",
    "path_dost = \"C:/Users/Professional/Desktop/dostNN/finetune\"\n",
    "for file in os.listdir(path_dost):\n",
    "    txt_dost += fil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726cc9c4-470d-4164-88b1-4ce4c56b393a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dost = torch.tensor(encode(txt), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c988a4-fa6f-400d-bbde-c1dd16cebf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(ttratio * len(data_dost))\n",
    "train_dost = data_dost[:n]\n",
    "val_dost = data_dost[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6679e-30a5-4d1c-b941-85361dbdee32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_dost(split):\n",
    "    data = train_dost if split=='train' else val_dost\n",
    "    idx = torch.randint(len(data_dost)-block_size, (batch_size,))\n",
    "    x = torch.stack([data_dost[i:i + block_size] for i in idx])\n",
    "    y = torch.stack([data_dost[i+1:i + block_size+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcbd6e-2317-43b5-a0ff-91db2ab9c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.AdamW(m2.parameters(), lr=1e-2)\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
    "\n",
    "batch_size = 128\n",
    "for steps in range(1000):\n",
    "    xb, yb = batch(\"train\")\n",
    "    xb_cuda = xb.to(device)\n",
    "    yb_cuda = yb.to(device)\n",
    "    logits, loss = m2(xb_cuda, yb_cuda)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if steps % 20 == 0:\n",
    "        sched.step()\n",
    "\n",
    "    if steps % 10 == 0:\n",
    "        print(f\"Step number: {steps}, loss: {loss.item()}, LR: {sched.get_last_lr()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
