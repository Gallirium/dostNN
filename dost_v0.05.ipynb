{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5373f9a0-efd6-49bd-af99-ddafd3be85e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e40b8b0-0efd-43ba-b027-6519e8ee33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = ''\n",
    "for file in os.listdir():\n",
    "    if file.endswith(\".txt\"):\n",
    "        with open(file, 'r', encoding=\"cp1251\") as f:\n",
    "            text = f.read()\n",
    "            txt += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6730f84c-0f7c-418b-aef9-f87aa765c113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6723492"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d046fc-c140-4652-9916-4673b7fa3110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\n",
      " !\"#&'()*,-./0123456789:;?@ABCDEFGHIJKLMNOPQRSTUVWXZ[]_abcdefghijklmnopqrstuvwxyz «»ІАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЪЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяёїќ–—’“”„…№\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "##Уникальные символы Достоевского\n",
    "chars = sorted(list(set(txt)))\n",
    "vocab_size=len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2263a4f3-b11b-4531-a462-0779f5bda42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##char level tokenizer\n",
    "stoi = { ch:i for i, ch in enumerate(chars)}\n",
    "itos = { i:ch for i, ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[c] for c in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c79114b-0bda-44ed-b819-a04a7d3d84ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6723492]) torch.int64\n",
      "tensor([109,  87, 103, 104, 113,   2, 101,  92, 102,  89,  87, 116,   1,   1,\n",
      "          1,   1,  90, 128, 117, 119, 117,   2, 132, 122, 133, 119, 117, 148,\n",
      "          1,   1,   1,   1,  37,   1,   1,  99, 122,   2, 136, 135, 122, 133,\n",
      "        132, 122, 119,  11,   2, 148,   2, 134, 122, 128,   2, 124, 117, 132,\n",
      "        125, 134, 144, 119, 117, 135, 145,   2, 146, 135, 136,   2, 125, 134,\n",
      "        135, 131, 133, 125, 147,   2, 129, 131, 125, 138,   2, 132, 122, 133,\n",
      "        119, 144, 138,   2, 141, 117, 120, 131, 119,   2, 130, 117,   2, 123,\n",
      "        125, 124, 130, 122, 130, 130, 131, 129,   2, 132, 131, 132, 133, 125,\n",
      "        142, 122,  11,   2, 135, 131, 120, 121, 117,   2, 127, 117, 127,   2,\n",
      "        129, 131, 120,   2, 118, 144,   2, 131, 118, 131, 126, 135, 125, 134,\n",
      "        145,   2, 125,   2, 118, 122, 124,   2, 135, 131, 120, 131,  13,   2,\n",
      "        100, 121, 130, 131,   2, 124, 130, 117, 147,   2, 130, 117, 119, 122,\n",
      "        133, 130, 131,  25,   2, 130, 125, 127, 131, 120, 121, 117,   2, 136,\n",
      "        123, 122,   2, 118, 131, 128, 122, 122,   2, 130, 122,   2, 134, 148,\n",
      "        121, 136,   2, 132, 125, 134, 117, 135, 145,   2, 129, 131, 147,   2,\n",
      "        117, 119, 135, 131, 118, 125, 131, 120, 133, 117, 137, 125, 147,  11,\n",
      "          2, 121, 117, 123, 122,   2, 122, 134, 128, 125,   2, 132, 133, 131,\n",
      "        123, 125, 119, 136,   2, 121, 131,   2, 134, 135, 117,   2, 128, 122,\n",
      "        135,  13,   2,  99, 117, 121, 131,   2, 118, 144, 135, 145,   2, 134,\n",
      "        128, 125, 141, 127, 131, 129,   2, 132, 131, 121, 128, 131,   2, 119,\n",
      "        128, 147, 118, 128, 122, 130, 130, 144, 129,   2, 119,   2, 134, 122,\n",
      "        118, 148,  11,   2, 140, 135, 131, 118, 144,   2, 132, 125, 134, 117,\n",
      "        135, 145,   2, 118, 122, 124,   2, 134, 135, 144, 121, 117,   2, 131,\n",
      "          2, 134, 117, 129, 131, 129,   2, 134, 122, 118, 122,  13,   2, 104,\n",
      "        122, 129,   2, 135, 131, 128, 145, 127, 131,   2, 134, 122, 118, 148,\n",
      "          2, 125, 124, 119, 125, 130, 148, 147,  11,   2, 140, 135, 131,   2,\n",
      "        130, 122,   2, 121, 128, 148,   2, 135, 131, 120, 131,   2, 132, 125,\n",
      "        141, 136,  11,   2, 121, 128, 148,   2, 140, 122, 120, 131,   2, 119,\n",
      "        134, 122,   2, 132, 125, 141, 136, 135,  11,   2, 135, 131,   2, 122,\n",
      "        134, 135, 145,   2, 130, 122,   2, 121, 128, 148,   2, 132, 131, 138,\n",
      "        119, 117, 128,   2, 140, 125, 135, 117, 135, 122, 128, 148,  13,   2,\n",
      "         92, 134, 128, 125,   2, 148,   2, 119, 121, 133, 136, 120,   2, 119,\n",
      "        124, 121, 136, 129, 117, 128,   2, 124, 117, 132, 125, 134, 117, 135,\n",
      "        145,   2, 134, 128, 131, 119, 131,   2, 119,   2, 134, 128, 131, 119,\n",
      "        131,   2, 119, 134, 122,  11,   2, 140, 135, 131,   2, 134, 128, 136,\n",
      "        140, 125, 128, 131, 134, 145,   2, 134, 131,   2, 129, 130, 131, 126,\n",
      "          2, 134,   2, 132, 133, 131, 141, 128, 131, 120, 131,   2, 120, 131,\n",
      "        121, 117,  11,   2, 135, 131,   2, 119, 124, 121, 136, 129, 117, 128,\n",
      "          2, 146, 135, 131,   2, 119, 134, 128, 122, 121, 134, 135, 119, 125,\n",
      "        122,   2, 119, 130, 136, 135, 133, 122, 130, 130, 122, 126,   2, 132,\n",
      "        131, 135, 133, 122, 118, 130, 131, 134, 135, 125,  25,   2, 121, 131,\n",
      "          2, 135, 131, 120, 131,   2, 148,   2, 132, 131, 133, 117, 123, 122,\n",
      "        130,   2, 119, 134, 122, 129,   2, 134, 131, 119, 122, 133, 141, 125,\n",
      "        119, 141, 125, 129, 134, 148,  13,   2, 116,   2, 124, 117, 132, 125,\n",
      "        134, 144, 119, 117, 147,   2, 128, 125, 141, 145,   2, 134, 131, 118,\n",
      "        144, 135, 125, 148,  11,   2, 136, 127, 128, 131, 130, 148, 148, 134,\n",
      "        145,   2, 119, 134, 122, 129, 125,   2, 134, 125, 128, 117, 129, 125,\n",
      "          2, 131, 135,   2, 119, 134, 122, 120, 131,   2, 132, 131, 134, 135,\n",
      "        131, 133, 131, 130, 130, 122, 120, 131,  11,   2, 117,   2, 120, 128,\n",
      "        117, 119, 130, 131, 122,   2, 153,   2, 131, 135,   2, 128, 125, 135,\n",
      "        122, 133, 117, 135, 136, 133, 130, 144, 138,   2, 127, 133, 117, 134,\n",
      "        131, 135,  26,   2, 128, 125, 135, 122, 133, 117, 135, 131, 133,   2,\n",
      "        132, 125, 141, 122, 135,   2, 135, 133, 125, 121, 139, 117, 135, 145,\n",
      "          2, 128, 122, 135,   2, 125,   2, 119,   2, 127, 131, 130, 139, 122,\n",
      "          2, 134, 131, 119, 134, 122, 129,   2, 130, 122,   2, 124, 130, 117,\n",
      "        122, 135,  11,   2, 121, 128, 148,   2, 140, 122, 120, 131,   2, 131,\n",
      "        130,   2, 132, 125, 134, 117, 128,   2, 134, 135, 131, 128, 145, 127,\n",
      "        131,   2, 128, 122, 135,  13,   2, 116,   2, 153,   2, 130, 122,   2,\n",
      "        128, 125, 135, 122, 133, 117, 135, 131, 133,  11,   2, 128, 125, 135,\n",
      "        122, 133, 117, 135, 131, 133, 131, 129,   2, 118, 144, 135, 145,   2,\n",
      "        130, 122,   2, 138, 131, 140, 136,   2, 125,   2, 135, 117, 142, 125,\n",
      "        135, 145,   2, 119, 130, 136, 135, 133, 122, 130, 130, 131, 134, 135,\n",
      "        145,   2, 121, 136, 141, 125,   2, 129, 131, 122, 126,   2, 125,   2,\n",
      "        127, 133, 117, 134, 125, 119, 131, 122,   2, 131, 132, 125, 134, 117,\n",
      "        130, 125, 122,   2, 140, 136, 119, 134, 135, 119,   2, 130, 117,   2,\n",
      "        125, 138,   2, 128, 125, 135, 122, 133, 117, 135, 136, 133, 130, 144,\n",
      "        126,   2, 133, 144, 130, 131, 127,   2, 132, 131, 140, 122, 128,   2,\n",
      "        118, 144,   2, 130, 122, 132, 133, 125, 128, 125, 140, 125, 122, 129,\n",
      "          2, 125,   2, 132, 131, 121, 128, 131, 134, 135, 145, 147,  13,   2,\n",
      "        103,   2, 121, 131, 134, 117, 121, 131, 126,  11,   2, 131, 121, 130,\n",
      "        117, 127, 131,  11,   2, 132, 133, 122, 121, 140, 136, 119, 134, 135,\n",
      "        119, 136, 147,  11,   2, 140])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(txt), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4dbed1-ac85-43f5-82e8-350ebbd44f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttratio = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe786da0-1a9a-48da-a719-eba192e5889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = int(ttratio * len(data))\n",
    "train = data[:n]\n",
    "val = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f0d678f-f01f-4f69-96d2-69b96eed4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 64\n",
    "block_size = 128\n",
    "\n",
    "def batch(split):\n",
    "    data = train if split=='train' else val\n",
    "    idx = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i + block_size] for i in idx])\n",
    "    y = torch.stack([data[i+1:i + block_size+1] for i in idx])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7e942cb-e33f-4a12-b734-6652c3e89530",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb, yb = batch('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac88624-5b3a-42a3-ad93-d3fa74fb5ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[122,  11,   2,  ..., 131,  11,  83],\n",
       "        [127, 117, 127,  ..., 127, 117, 129],\n",
       "        [  2, 125,   2,  ...,   2, 118, 122],\n",
       "        ...,\n",
       "        [119,   2, 140,  ..., 144,   2, 132],\n",
       "        [131,   2, 146,  ..., 128, 125,   2],\n",
       "        [119, 117, 128,  ..., 133, 122,  13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6355d8f3-9b8f-45b2-9d80-faf1e14b4b40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 11,   2, 130,  ...,  11,  83, 152],\n",
       "        [117, 127,  12,  ..., 117, 129, 125],\n",
       "        [125,   2, 135,  ..., 118, 122, 134],\n",
       "        ...,\n",
       "        [  2, 140, 122,  ...,   2, 132, 133],\n",
       "        [  2, 146, 135,  ..., 125,   2, 136],\n",
       "        [117, 128, 125,  ..., 122,  13,   2]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2333850e-e0d6-4b5a-84c3-56ba4a318e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.640864 M parameters\n",
      "torch.Size([8192, 160])\n",
      "tensor(5.2414, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "##And now, self-attention\n",
    "\n",
    "\n",
    "n_layer = 8\n",
    "n_embd = 128\n",
    "n_head = 32\n",
    "\n",
    "dropout = 0.2\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size,block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        wei = q @ k.transpose(-2, -1) * C **-0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        v = self.value(x)\n",
    "        out = wei @ v\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedForward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class BigramLM2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[TransformerBlock(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd)\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) ##(b, t, c)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(t, device=device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = self.lm_head(x).to(device) ##(b, t, vocab_size)\n",
    "\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            b, t, c = logits.shape\n",
    "            logits = logits.view(b*t, c).to(device)\n",
    "            targets = targets.view(b*t).to(device)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets).to(device)\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            logits, loss = self(idx_cond)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "        return idx.to(device)\n",
    "\n",
    "xb_cuda = xb.to(device)\n",
    "yb_cuda = yb.to(device)\n",
    "\n",
    "m2 = BigramLM2().to(device)\n",
    "print(sum(p.numel() for p in m2.parameters())/1e6, \"M parameters\")\n",
    "logits, loss = m2(xb_cuda, yb_cuda)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d68adf53-a13b-4b72-9868-5151f329fb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step number: 0, loss: 5.243777751922607, LR: [0.0027]\n",
      "Step number: 100, loss: 2.6064109802246094, LR: [0.0027]\n",
      "Step number: 200, loss: 2.534100294113159, LR: [0.0024300000000000003]\n",
      "Step number: 300, loss: 2.238802194595337, LR: [0.0024300000000000003]\n",
      "Step number: 400, loss: 2.0308139324188232, LR: [0.002187]\n",
      "Step number: 500, loss: 1.8573051691055298, LR: [0.002187]\n",
      "Step number: 600, loss: 1.8129334449768066, LR: [0.0019683]\n",
      "Step number: 700, loss: 1.7411245107650757, LR: [0.0019683]\n",
      "Step number: 800, loss: 1.6971685886383057, LR: [0.00177147]\n",
      "Step number: 900, loss: 1.6656464338302612, LR: [0.00177147]\n",
      "Step number: 1000, loss: 1.6415959596633911, LR: [0.0015943230000000001]\n",
      "Step number: 1100, loss: 1.6181708574295044, LR: [0.0015943230000000001]\n",
      "Step number: 1200, loss: 1.5791606903076172, LR: [0.0014348907]\n",
      "Step number: 1300, loss: 1.5825529098510742, LR: [0.0014348907]\n",
      "Step number: 1400, loss: 1.5791224241256714, LR: [0.00129140163]\n",
      "Step number: 1500, loss: 1.5874698162078857, LR: [0.00129140163]\n",
      "Step number: 1600, loss: 1.5754334926605225, LR: [0.001162261467]\n",
      "Step number: 1700, loss: 1.546882152557373, LR: [0.001162261467]\n",
      "Step number: 1800, loss: 1.5091097354888916, LR: [0.0010460353203000001]\n",
      "Step number: 1900, loss: 1.5105642080307007, LR: [0.0010460353203000001]\n",
      "Step number: 2000, loss: 1.51740300655365, LR: [0.0009414317882700001]\n",
      "Step number: 2100, loss: 1.4790890216827393, LR: [0.0009414317882700001]\n",
      "Step number: 2200, loss: 1.4766583442687988, LR: [0.0008472886094430002]\n",
      "Step number: 2300, loss: 1.4784122705459595, LR: [0.0008472886094430002]\n",
      "Step number: 2400, loss: 1.495686650276184, LR: [0.0007625597484987002]\n",
      "Step number: 2500, loss: 1.4596055746078491, LR: [0.0007625597484987002]\n",
      "Step number: 2600, loss: 1.471290946006775, LR: [0.0006863037736488302]\n",
      "Step number: 2700, loss: 1.4696855545043945, LR: [0.0006863037736488302]\n",
      "Step number: 2800, loss: 1.446960210800171, LR: [0.0006176733962839472]\n",
      "Step number: 2900, loss: 1.4286038875579834, LR: [0.0006176733962839472]\n",
      "Step number: 3000, loss: 1.4400662183761597, LR: [0.0005559060566555524]\n",
      "Step number: 3100, loss: 1.4271069765090942, LR: [0.0005559060566555524]\n",
      "Step number: 3200, loss: 1.4126418828964233, LR: [0.0005003154509899972]\n",
      "Step number: 3300, loss: 1.4574370384216309, LR: [0.0005003154509899972]\n",
      "Step number: 3400, loss: 1.4472510814666748, LR: [0.00045028390589099747]\n",
      "Step number: 3500, loss: 1.422316551208496, LR: [0.00045028390589099747]\n",
      "Step number: 3600, loss: 1.4311177730560303, LR: [0.0004052555153018977]\n",
      "Step number: 3700, loss: 1.4230742454528809, LR: [0.0004052555153018977]\n",
      "Step number: 3800, loss: 1.4639924764633179, LR: [0.00036472996377170795]\n",
      "Step number: 3900, loss: 1.4218544960021973, LR: [0.00036472996377170795]\n",
      "Step number: 4000, loss: 1.4073255062103271, LR: [0.00032825696739453717]\n",
      "Step number: 4100, loss: 1.420461893081665, LR: [0.00032825696739453717]\n",
      "Step number: 4200, loss: 1.436012625694275, LR: [0.00029543127065508344]\n",
      "Step number: 4300, loss: 1.4270271062850952, LR: [0.00029543127065508344]\n",
      "Step number: 4400, loss: 1.408645510673523, LR: [0.0002658881435895751]\n",
      "Step number: 4500, loss: 1.4361178874969482, LR: [0.0002658881435895751]\n",
      "Step number: 4600, loss: 1.426329255104065, LR: [0.0002392993292306176]\n",
      "Step number: 4700, loss: 1.3970050811767578, LR: [0.0002392993292306176]\n",
      "Step number: 4800, loss: 1.4206269979476929, LR: [0.00021536939630755584]\n",
      "Step number: 4900, loss: 1.4430021047592163, LR: [0.00021536939630755584]\n",
      "Step number: 5000, loss: 1.3839526176452637, LR: [0.00019383245667680025]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m xb_cuda \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m yb_cuda \u001b[38;5;241m=\u001b[39m yb\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m \u001b[43mm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxb_cuda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myb_cuda\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m optim\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 87\u001b[0m, in \u001b[0;36mBigramLM2.forward\u001b[0;34m(self, idx, targets)\u001b[0m\n\u001b[1;32m     85\u001b[0m pos_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding_table(torch\u001b[38;5;241m.\u001b[39marange(t, device\u001b[38;5;241m=\u001b[39mdevice))\n\u001b[1;32m     86\u001b[0m x \u001b[38;5;241m=\u001b[39m tok_emb \u001b[38;5;241m+\u001b[39m pos_emb\n\u001b[0;32m---> 87\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_f(x)\n\u001b[1;32m     89\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(x)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m##(b, t, vocab_size)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 68\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 68\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msa\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mln1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffwd(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln2(x))\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(\u001b[43m[\u001b[49m\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheads\u001b[49m\u001b[43m]\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "Cell \u001b[0;32mIn[13], line 39\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 39\u001b[0m     out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[43mh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheads], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     40\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj(out))\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optim = torch.optim.AdamW(m2.parameters(), lr=3e-3)\n",
    "sched = torch.optim.lr_scheduler.ExponentialLR(optim, gamma=0.9)\n",
    "\n",
    "batch_size = 128\n",
    "for steps in range(5000):\n",
    "    xb, yb = batch(\"train\")\n",
    "    xb_cuda = xb.to(device)\n",
    "    yb_cuda = yb.to(device)\n",
    "    logits, loss = m2(xb_cuda, yb_cuda)\n",
    "    optim.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if steps % 200 == 0:\n",
    "        sched.step()\n",
    "\n",
    "    if steps % 100 == 0:\n",
    "        print(f\"Step number: {steps}, loss: {loss.item()}, LR: {sched.get_last_lr()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7dffe738-85e0-4984-ade0-28da722dfde4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tЭто есть! – пробовал вдруг Нину, которого остащив и и про удовольствия, – в отржанившийся нагло, хоть со стороны уже последний ввечер и по сапожом Петра Петровича, – пробавил Васин коли почти. Говорил он из тополу укусил, что ыстановлялся в почь улыбнулся ей денег, но отчетая Алексей. Ибо мне касается останется выше на стол, чтобы они не только были извсех и одеяло приготовее, и… собой? (и вы по порпашу\".\n",
      "\n",
      "– Ну, не только удараетесь, – отвечал крикованький и рот Марья, – правню той неизвидений сюач, на света-с. Хохлактива, так пронюблю поверительность сознать. Соймет милый ей выйдет в то же время, оза что теперь самый друг полый, к азношений чувств, он отчеб будто сдуновался хотел всё долом.\n",
      "\n",
      "- И что именно?\n",
      "\n",
      "- Оробеть; дома, Лебядкин еще наемное, - обракалась Лизавета! - кроме в последних, вдруг сделающих грудих и кипел паннюю, все находящую (чем нашам готов было условно захлоее пожалеь, и выротили за его), по 1860 - готовую часа, «ю, когда читали в буюшу» волосов и вообще под их помочь», говорил тонко кто прямо.\n",
      "\n",
      "Липутин и взял его первышем тялался и уже как бы червский коробад, чем быть не мог наверо. Дело, что ж уж с тех пор позонится песматривалась о приправда ему очно думала, какой потомут впечатление много, с подозрения и всем упомянул, даже тум новым в беспокойством машущенствально к тому! Тем говорил-то, Совута же приду и Шурдовский; даже вам того похоже обидовый, — чуть не пременка мысла его ждами не только пустили, вы, почти же были старичонко, Аглая, а не любовь. Несколько встала в реадости навстречивала; голова в лице ее и непременное послала этими князю, одно подняла и умрокил была, но взбула поставняя изостинь в то время предворядилась этим \"хитрым, таким несмотря упададывал\", и с того, везую без того хорошо щувы; черт ли ежедневно прекраснею себе, это настоящую честь мы точнув своим уже возможно искасывать их просто ли слышалуся. Но Лизавета Версилова раз, что Феняну, реввожность завещали молча по вечерах действительно сдерживают всем. Но кричали — у\n"
     ]
    }
   ],
   "source": [
    "context = torch.zeros((1,1), dtype=torch.long, device=device)\n",
    "print(decode(m2.generate(context,max_new_tokens=2000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d57e401-8d77-4cfa-8e9d-5adca4fab198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.419751763343811"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9cae92b-7a0e-4e64-8fc2-75cba6daf05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    xt, yt = batch(\"val\")\n",
    "    logt, losst = m2(xt.to(device), yt.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9c5af1c-5f13-47b3-a236-a3f77ced9ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.4779, device='cuda:0')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34eef1ba-f68b-402a-9799-b94455981e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 128])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fcbd6e-2317-43b5-a0ff-91db2ab9c89f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
